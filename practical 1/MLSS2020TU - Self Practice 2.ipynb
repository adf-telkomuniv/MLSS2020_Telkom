{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLSS2020TU - Self Practice 2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPM6+n+4nN/1dIlNA6/ul0w"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SB0EeXzyu_sz","colab_type":"text"},"source":["# Self Practice 2: Multilayer Perceptron using Numpy\n","Â© Machine Learning Summer School - Telkom University\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN","colab_type":"text"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\" border=\"0\"><tr><td>\n","\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Self%20Practice%202.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=\"20px\" alt=\"open in colab\"/></a>\n","\n","</td><td>&nbsp;|&nbsp;\n","</td><td>\n","<a target=\"_blank\" href=\"https://github.com/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Self%20Practice%202.ipynb\"><img src=\"https://i.ibb.co/GR67738/pinpng-com-github-logo-png-945585.png\" height=\"20px\" /></a>\n","</td></tr></table>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqiFD42_92oE"},"source":["\n","In this exercise you will practice putting together a simple image classification pipeline, based on Neural Network classifier. \n","\n","The goals of this assignment are as follows:\n","\n","1. understand the basic Multi-class Classification pipeline using Multilayer Perceptron\n","1. implement implement Deep Neural Network API\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vXUed3x_ROCm","colab_type":"text"},"source":["\n","# [Part 0] Preparation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wwfchKW2tnuH","colab_type":"text"},"source":["## 1 - Import Libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JLQflrey92oM","colab":{}},"source":["import numpy as np\n","\n","from PIL import Image\n","import cv2 as cv\n","import h5py    \n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","np.set_printoptions(precision=4)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K86RgAJltrKE","colab_type":"text"},"source":["## 2 - Helper Functions"]},{"cell_type":"markdown","metadata":{"id":"-5lSPWUB5ZmP","colab_type":"text"},"source":["---\n","# [Part 1] Neural Network"]},{"cell_type":"markdown","metadata":{"id":"5dnhJbhciwXG","colab_type":"text"},"source":["## Single Layer Perceptron\n","As we've seen in previous exercises, Single Layer Perceptron is essentially a Linear Classifier. With only one layer in the network, the architecture illustration is as described below\n","\n","\n","![onelayer](https://image.ibb.co/fjR3oz/onelayer.png) \n"]},{"cell_type":"markdown","metadata":{"id":"NcIi2S-UiwXH","colab_type":"text"},"source":["## Multi Layer Perceptron\n","We can further stacks the layers of neuron into a deeper architecture called Multi-Layer Perceptron (MLP). Layers located between input and output layer are called hidden layers.\n","\n","MLP with 2 neuron layers called *2-Layer Neural Network* or *1-Hidden Neural Network*. The same apply for MLP with 3 layers called *3-Layer Neural Net* or *2-Hidden Neural Net*. Below are the illustration of 2-layer and 3-layer net \n","\n","*2-layer NN* | *3-layer NN*\n","- | -\n","![2layerNN](https://image.ibb.co/dHnnFe/2layerNN.png) | ![3layerNN](https://image.ibb.co/iH18MK/3layerNN.png)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fPjmBVeoiwXI","colab_type":"text"},"source":["---\n","## Backpropagation\n","You've also seen this in previous exercises, that in learning a Neural Network using Gradient Descent, there are several steps to be made:\n","* forward pass to multipy weights and input\n","* calculate error\n","* backward pass to get the input gradients and weights gradients\n","\n","    \n","If we implement it in a simple python, the code for Single Layer Perceptron will need just several lines of code as follow:\n","```python\n","for epoch in range(max_epoch):\n","    \n","    layer = np.dot(feature, weight)\n","    activation = 1 / (1 + np.exp(-layer))    \n","\n","    error = target - activation\n","\n","    g_activation = (err) * (activation * (1 - activation))\n","    g_weight = feature.T.dot(layer)\n","    \n","    weight = weight + lr*g_bobot\n","```\n","\n","You'll notice that to train Multi Layered Perceptron is essentially repeating the forward pass for each layer, continued by repeating reversely backward pass through each layer. \n","\n","We can implement each prward/backward pass for every specific architecture, but that will be too wastefull. Instead, we can build several API functions so that we can easily add or remove layers in an architecture."]},{"cell_type":"markdown","metadata":{"id":"GXX0qY6-mNUj","colab_type":"text"},"source":["---\n","# [Part 2] More Compact API\n","In the previous exercise you implemented a single layer perceptron on CIFAR-10. The implementation was simple but not very modular.\n","\n","Implementing functional API to build and train Deep Neural Network is what have been done by popular Deep Learning Library and frameworks such as Keras, Tensorflow, and Torch\n","\n","In this exercise we will implement a `forward` and a `backward` function for fully-connected networks using a more compact modular approach. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n","\n","```python\n","def layer_forward(x, w):\n","  \"\"\" Receive inputs x and weights w \"\"\"\n","  # Do some computations ...\n","  z = # ... some intermediate value\n","  # Do some more computations ...\n","  out = # the output\n","   \n","  cache = (x, w, z, out) # Values we need to compute gradients\n","   \n","  return out, cache\n","```\n","\n","The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n","\n","```python\n","def layer_backward(dout, cache):\n","  \"\"\"\n","  Receive dout (derivative of loss with respect to outputs) and cache,\n","  and compute derivative with respect to inputs.\n","  \"\"\"\n","  # Unpack cache values\n","  x, w, z, out = cache\n","  \n","  # Use values in cache to compute derivatives\n","  dx = # Derivative of loss with respect to x\n","  dw = # Derivative of loss with respect to w\n","  \n","  return dx, dw\n","```\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cqPr5kaVuEd6"},"source":["\n","\n","For this part, we've laready provide you the implementation of some basic layers"]},{"cell_type":"markdown","metadata":{"id":"oLrwVZt2mNUk","colab_type":"text"},"source":["---\n","##1 - Affine API\n","\n","Implement functions to compute the forward and backward pass foran affine (fully-connected) layer."]},{"cell_type":"code","metadata":{"id":"BnVusViAmNUl","colab_type":"code","colab":{}},"source":["def affine_forward(x, W, b ):  \n","    \"\"\"\n","    Inputs:\n","    - x : A numpy array containing input data, of shape (N, d_1, ..., d_k)\n","    - W : A numpy array of weights, of shape (D, M)\n","    - b : A numpy array of biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - out  : output, of shape (N, M)\n","    - cache: (x, w, b)\n","    \"\"\"\n","\n","    v = np.dot(x, W) + b    \n","    cache = (x, W, b)\n","    \n","    return v, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spWWmF1HmNUq","colab_type":"code","colab":{}},"source":["def affine_backward(dout, cache):\n","    \"\"\"\n","    Inputs:\n","    - dout: Upstream derivative, of shape (N, M)\n","    - cache: Tuple of:\n","      - x : Input data, of shape (N, d_1, ... d_k)\n","      - w : Weights, of shape (D, M)\n","      - b : Biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - dx  : Gradient with respect to x, of shape (N, d1, ..., d_k)\n","    - dw  : Gradient with respect to w, of shape (D, M)\n","    - db  : Gradient with respect to b, of shape (M,)\n","    \"\"\"\n","\n","    x, W, b = cache\n","    dW = np.dot(x.T,dout)\n","    db = np.sum(dout, axis=0, keepdims=True)\n","    dx = dout.dot(W.T)\n","    \n","    return dW, db, dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NJrRDsp8nIRH","colab_type":"text"},"source":["##2 - ReLU API\n","\n","Implement functions to compute the forward and backward pass for a layer of rectified linear units (ReLUs)"]},{"cell_type":"code","metadata":{"id":"5zXo1d5YnIRK","colab_type":"code","colab":{}},"source":["def relu_forward(x):\n","    \"\"\"\n","    Input:\n","    - x : Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out  : Output, of the same shape as x\n","    - cache: x\n","    \"\"\"    \n","\n","    out = x * (x > 0).astype(float)\n","    cache = x\n","    \n","    return out, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FYKK3-y2rVla","colab_type":"code","colab":{}},"source":["def relu_backward(dout, cache):\n","    \"\"\"\n","    Input:\n","    - dout  : Upstream derivatives, of any shape\n","    - cache : Input x, of same shape as dout\n","\n","    Returns:\n","    - dx : Gradient with respect to x\n","    \"\"\"\n","  \n","    dx = dout * (cache >= 0)\n","    \n","    return dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-k8-FNOmNU7","colab_type":"text"},"source":["##3 - Softmax API\n","\n","Implement function to compute the softmax score and function to compute the loss and gradient for softmax classification."]},{"cell_type":"code","metadata":{"id":"x_9sC3jFmNU9","colab_type":"code","colab":{}},"source":["def softmax(x):  \n","    \"\"\"\n","    Inputs:\n","    - x: Input data, of shape (N, C) \n","         where x[i, j] is the score for the jth class for the ith input.\n","\n","    Returns a tuple of:\n","    - score : softmax score, of shape (N, C) (normalize log-probability score)\n","    \"\"\"\n","  \n","    x -= np.max(x)\n","    x_exp = np.exp(x)\n","    x_sum = np.sum(x_exp, axis = 1, keepdims = True)  \n","    score = x_exp / x_sum\n","    \n","    return score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWes0OWamNVD","colab_type":"code","colab":{}},"source":["def softmax_loss(score, y):\n","    \"\"\"\n","    Inputs:\n","    - score : softmax score, of shape (N, C)\n","    - y     : Vector of labels, of shape (N,) \n","              where y[i] is the label for x[i] and 0 <= y[i] < C\n","\n","    Returns a tuple of:\n","    - loss  : Scalar giving the loss\n","    - dx    : Gradient of the loss with respect to x\n","    \"\"\"\n","   \n","    num_examples = score.shape[0]\n","    number_list = range(num_examples)\n","    corect_logprobs = -np.log(score[number_list,y])\n","    loss = np.sum(corect_logprobs)/num_examples\n","    \n","    dscores = score\n","    dscores[range(num_examples),y] -= 1\n","    dscores /= num_examples\n","\n","    \n","    return loss, dscores"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMkFCF8kiwXl","colab_type":"text"},"source":["---\n","#[Part 3] Deep Neural Net API\n","\n","With the API implementation is sorted, we can now easily build the Neural Network. We'll create a Deep Neural Net API which will automatically create deep layers for a given list of hidden layer\n","\n","With this, we can create however many layers in our architecture. To implement a one training epoch for Single Layer Perceptron, we just need to stack these functions\n","\n","<pre><font color='green'>affine_fwd</font> -> \n","    <font color='blue'>sigmoid_fwd</font> ->\n","        <font color='red'>calculate_loss</font> ->\n","    <font color='blue'>sigmoid_bwd</font> -> \n","<font color='green'>affine_bwd</font> -> \n","weights_update</pre>\n","\n","---\n","Then, to build a 2 Layer Neural Net (1 hidden layer), we only need to add several functions \n","<pre><font color='green'>affine_fwd</font> -> \n","    <font color='blue'>sigmoid_fwd</font> ->\n","        <font color='green'>affine_fwd</font> -> \n","            <font color='blue'>sigmoid_fwd</font> ->\n","                <font color='red'>calculate_loss</font> ->\n","            <font color='blue'>sigmoid_bwd</font> -> \n","        <font color='green'>affine_bwd</font> -> \n","    <font color='blue'>sigmoid_bwd</font> -> \n","<font color='green'>affine_bwd</font> -> \n","weights_update</pre>\n","\n","Let's try it"]},{"cell_type":"markdown","metadata":{"id":"iCloT9NVmNVI","colab_type":"text"},"source":["---\n","## 1 - Weight Init\n","Below is a function to repeatedly initialize weights and bias for each layer"]},{"cell_type":"code","metadata":{"id":"yQe8zlZfmNVK","colab_type":"code","colab":{}},"source":["def init_weights(d_in, hidden, d_out, std=1e-2, seed=None):\n","    \"\"\"\n","    Inputs:\n","    - d_in  : int, number of input dimension\n","    - hidden: list of number hidden neuron in each hiidden layer\n","    - d_out : int, number of output dimension\n","    - std   : standar deviation for generating weights\n","    - seed  : random seed\n","    \n","    Outputs:\n","    - W: list of Weights\n","    - b: list of biases\n","    \"\"\"\n","    \n","    W = []\n","    b = []\n","    np.random.seed(seed)\n","    dims = [d_in] + hidden + [d_out] \n","    \n","    for i in range(len(dims)-1):\n","        W.append(std * np.random.randn(dims[i],dims[i+1]))\n","        b.append(np.zeros((1, dims[i+1])))\n","    return W, b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JTzrXg8nmNVZ","colab_type":"text"},"source":["---\n","## 2 - Predict Function\n","\n","Implement the predict function first, because we are going to use **predict** function inside the **training** function to track the **validation** accuracy \n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - <font color=\"blue\">N * [FC Layer - activation]</font> - FC Layer - argmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XSugv9vVmNVb","colab_type":"text"},"source":["#### <font color='red'>**EXERCISE**: </font>\n","**Implement Predict Function**\n","\n","    * loop call forward function for each hidden layer weights\n","    * check and use the requested activation funtion\n","    * call forward function for the last layer"]},{"cell_type":"code","metadata":{"id":"p3_ufLTHmNVd","colab_type":"code","colab":{}},"source":["def predict_multi_layer(X, W, b):    \n","    \"\"\"\n","    Inputs:\n","    - X    : Input data, of shape(N, D)\n","    - W    : list of Weight\n","    - b    : list of biases\n","    \n","    Output:\n","    - y_pred : list of class prediction\n","    \"\"\"\n","    y_pred = np.zeros(X.shape[1])\n","    n_layer = len(W)\n","    \n","    # first activation is X\n","    act = X\n","    \n","    ## ------------------------- start your code here -------------------------\n","    \n","    # loop i over n_layer-1\n","    for i in range(n_layer-1):\n","    \n","        # calculate layer score by calling affine forward function using act, W[i], and b[i]\n","        layer, _ = ??\n","  \n","        # calculate activation score by calling relu forward function using layer score\n","        act, _ = ??\n","          \n","\n","    # calculate last layer score by calling affine forward function using act, W[-1], and b[-1]\n","    last_layer, _ = ??\n","    \n","    \n","    # take the maximum prediction from the last layer and use that column to get the class       \n","    # use np.argmax with axis=-1 \n","    y_pred = ??\n","\n","    ## ------------------------- end your code here -------------------------\n","    \n","    return y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZLv7H7WXmNVo","colab_type":"text"},"source":["---\n","## 3 - Training Function\n","\n","Now let's complete the training function\n","\n","\n","<br>\n","\n","The network architecture should be: \n","<pre><b>Input - <font color=\"blue\">N * [FC Layer - activation]</font> - FC Layer - Softmax</b></pre>\n","\n","<br>\n","\n","The **N** is the number of hidden layer, which can be calculated from **len(W)-1**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"56ExupMlmNVp","colab_type":"text"},"source":["<br>\n","\n","#### <font color='red'>**EXERCISE**: </font>\n","**Implement Training Function**\n","\n","there are **four steps** in this training function\n","\n","---\n","\n","**1. Forward Pass**\n","\n","    * loop over hidden layer [for len(W)-1]\n","        * call affine forward function\n","        * call activation forward function\n","    * call affine forward function for the last layer\n","    * call softmax score function\n","\n","**2. Calculate Loss**\n","\n","    * call softmax_loss function\n","    * loop over weights [for W]\n","        * calculate loss with regularization\n","\n","\n","**3. Backward Pass**\n","\n","    * call affine backward function for the last layer\n","    * loop over hidden layer [from len(W)-2 to 0]\n","        * call activation backward function\n","        * call affine backward function\n","\n","**4. Weight Update**\n","\n","    * loop over weights [for W]\n","        * implement weight update\n","    * calculate the training and validation accuracy"]},{"cell_type":"code","metadata":{"id":"XokPEJ3LmNVr","colab_type":"code","colab":{}},"source":["def train_multi_layer(X, y, X_val, y_val, hidden_size, \n","                      W=None, b=None, std=1e-4, seed=None,\n","                      lr=1e-4, lr_decay=0.95, reg=0.25, \n","                      epochs=100, batch_size=200, verbose=True):\n","    \"\"\"\n","    Inputs:\n","    - X          : array of train data, of shape (N, D)\n","    - y          : array of train labels, of shape (N,)\n","    - X_val      : array of validation data, of shape (Nv, D)\n","    - y_val      : array of validation labels, of shape (Nv,)\n","    - hidden_size: list of hidden neuron for each hidden layer\n","    - W          : list of Weight, if W is None, it will be initialized\n","    - b          : list of biases, if W is None, bias will be initialized\n","    - std        : float, standar deviation for generating weights\n","    - seed       : int, initial random seed\n","    - lr         : float, initial learning rate\n","    - lr_decay   : float, 0-1, decay rate to reduce learning rate each epoch\n","    - reg        : float, regularization rate\n","    - epochs     : int, number of training epoch\n","    - batch_size : int, number of batch used each step\n","    - verbose    : boolean, verbosity\n","    \n","    Outputs:\n","    - W          : list of trained Weights\n","    - b          : list of trained biases\n","    - history    : list of training history [loss, train_acc, val_acc]\n","    \n","    \"\"\"\n","    \n","    num_train, dim = X.shape\n","    \n","    \n","    # check if data train is divisible by batch size\n","    assert num_train % batch_size==0, \"data train \"+str(num_train)+\" is not divisible by batch size\"+str(batch_size)\n","    \n","    # total iteration per epoch\n","    num_iter = num_train // batch_size\n","    \n","    #start iteration counts\n","    it = 0\n","    \n","    # assume y takes values 0...K-1 where K is number of classes\n","    num_classes = np.max(y) + 1  \n","        \n","    # initialize Weights\n","    if W is None:\n","        W, b = init_weights(dim, hidden_size, num_classes, std, seed) \n","        \n","    # number of layer (including output layer)\n","    n_layer = len(W)\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","    train_acc_history = []\n","    val_acc_history = []\n","    \n","    \n","    ## ------------------------- start your code here -------------------------\n","\n","    print('start training')\n","    for ep in range(epochs):\n","\n","        # Shuffle data train index\n","        train_rows = np.arange(num_train)\n","        np.random.shuffle(train_rows)\n","        \n","        # split index into mini batches\n","        id_batch = np.split(train_rows, num_iter)\n","  \n","        for batch in id_batch:\n","      \n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","\n","            # store all cache in dictionary\n","            cache = {}\n","\n","            # first layer activation input is X_batch\n","            act = X_batch\n","\n","            # ------------------------------------------------\n","            # 1. Forward Pass\n","            # ------------------------------------------------\n","\n","            # loop i over hidden layer (n_layer-1)\n","            # see predict function implementation\n","            for i in range(n_layer-1):\n","\n","                # calculate layer score by calling affine forward function using activation act, W[i], and b[i]\n","                layer, cache_affine = ??\n","\n","                # calculate activation score by calling relu forward function using layer score\n","                act, _ = ??\n","\n","                # combine cache from affine and activation layer into cache for this layer\n","                cache[i] = (cache_affine, cache_act)\n","\n","            # calculate last layer score by calling affine forward function using activation act, W[i+1], and b[i+1]\n","            last_layer, cache[i+1] = ??\n","\n","            # calculate softmax score by calling softmax function using last_layer score\n","            softmax_score = ??\n","\n","            # ------------------------------------------------\n","            # 2. Calculate Loss\n","            # ------------------------------------------------\n","\n","            # evaluate loss and gradient by calling softmax_loss function using input softmax_score and y_batch\n","            loss, dout = ??\n","\n","            # add regularization to the loss:\n","            #  for each weights, calculate the sum square, multiply regularization strength\n","            #  then add it to the loss\n","            for w in W:\n","                loss += reg * np.sum(w*w)\n","\n","            # append the loss history\n","            loss_history.append(loss)\n","\n","\n","            # ------------------------------------------------\n","            # 3. Backward Pass\n","            # ------------------------------------------------    \n","\n","            # dictionary to contain all gradients\n","            dW = {}\n","            db = {}\n","\n","            # calculate last weights gradient by calling affine backward function using dout and cache[n_layer-1]\n","            dW[n_layer-1], db[n_layer-1], dact = affine_backward(dout, cache[n_layer-1])\n","\n","            #loop i from n_layer-2 down to 0\n","            for i in range(n_layer-2,-1,-1):\n","\n","                # extract affine cache and activation cache from layer cache\n","                cache_affine, cache_act = cache[i]\n","\n","                # calculate relu gradient by calling relu backward function using dact and cache_act score\n","                dlayer = ??\n","\n","                # calculate layer weights gradient by calling affine backward function using dlayer and cache_affine\n","                dW[i], db[i], dact = ??\n","\n","                # add regularization to gradient\n","                dW[i] += 2 * reg * W[i]\n","\n","            # ------------------------------------------------\n","            # 4. Weight Update\n","            # ------------------------------------------------    \n","\n","            # perform parameter update by subtracting W[i] and b[i] for each layer with a fraction of dW[i] and db[i]\n","            # according to the learning rate\n","            for i in range(len(W)):    \n","                W[i] -= lr * dW[i]\n","                b[i] -= lr * db[i]\n","\n","            # iteration count\n","            it +=1\n","\n","            if verbose and it % 100 == 0:\n","                print ('iteration',it,'(epoch', ep,'/',epochs, '): loss =', loss)\n","              \n","            \n","        # At the end of one epoch\n","        # 1. Check accuracy\n","        #    calculate the training accuracy by calling predict_multi_layer function on X_batch\n","        #    and compare it to y_batch. Then calculate the mean correct (accuracy in range 0-1)\n","        train_acc = (predict_multi_layer(X_batch, W, b) == y_batch).mean()\n","        train_acc_history.append(train_acc)\n","\n","        # 2. Calculate the training accuracy by calling predict_multi_layer function on X_val\n","        #    and compare it tu y_val. Then calculate the mean correct (accuracy in range 0-1)\n","        val_acc = (predict_multi_layer(X_val, W, b) == y_val).mean()\n","        val_acc_history.append(val_acc)\n","\n","        # 3. Decay learning rate\n","        #    multiply learning rate with decay\n","        lr *= lr_decay\n","            \n","            \n","    ## ------------------------- end your code here -------------------------\n","    \n","    history = [loss_history, train_acc_history, val_acc_history]\n","    \n","    if verbose:\n","      print('Training done')\n","    \n","    return W, b, history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dOXrvo7nmNVu","colab_type":"text"},"source":["---\n","# [Part 4] CIFAR-10 Dataset\n","\n","Here, we'll use the CIFAR-10 dataset\n","\n","for that, let's load and preprocess it first"]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","## 1 - Load CIFAR-10"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6YNXtE039Dmd"},"source":["---\n","## 2 - Visualizing Data\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oh3fg51s9Dmf","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","label = y_train.ravel()\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train_ori[i+j*10])\n","        ax[j,i].set_title(class_names[label[i+j*10]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 3 - Split Validation Data"]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","colab":{}},"source":["X_val_ori   = X_train_ori[-10000:,:]\n","y_val       = y_train[-10000:]\n","\n","X_train_ori = X_train_ori[:-10000, :]\n","y_train     = y_train[:-10000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 4 - Normalize and Reshape Data"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","colab":{}},"source":["X_train  = X_train_ori.astype('float32')\n","X_val    = X_val_ori.astype('float32')\n","X_test   = X_test_ori.astype('float32')\n","\n","mean_image = np.mean(X_train, axis = 0)\n","\n","X_train -= mean_image\n","X_val   -= mean_image\n","X_test  -= mean_image\n","\n","X_train  = X_train.reshape((X_train.shape[0],X_train.shape[1]*X_train.shape[2]*X_train.shape[3]))\n","X_val    = X_val.reshape((X_val.shape[0],X_val.shape[1]*X_val.shape[2]*X_val.shape[3]))\n","X_test   = X_test.reshape((X_test.shape[0],X_test.shape[1]*X_test.shape[2]*X_test.shape[3]))\n","\n","print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)\n","\n","y_train  = y_train.ravel()\n","y_val    = y_val.ravel()\n","y_test   = y_test.ravel()\n","\n","print('\\ny_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5t2TWycmNWU","colab_type":"text"},"source":["---\n","#[Part 5] Deep Neural Network\n","* In this part, we'll increase the network capacity by increasing the number of hidden layers\n"," \n","* Like before, we'll try both **tanh** and **relu** activation function, then compare the results\n"," \n","* And since we're using more layers, we'll train it a little bit longer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vc7LETzCUYMQ"},"source":["---\n","##1 - Train Network"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"BBAJlDlAUYMY","colab":{}},"source":["hidden_size=[50, 50]\n","\n","W, b, history = train_multi_layer(\n","    X_train, y_train, X_val, y_val, \n","    hidden_size=hidden_size, \n","    std=1e-2, lr=1e-2,\n","    lr_decay=0.95, reg=0.01, \n","    epochs=20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"NWrj-pbiUYMh"},"source":["---\n","##2 - Visualize Training\n","Visualize the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"scrolled":false,"colab_type":"code","id":"9C6Su2WQUYMj","colab":{}},"source":["loss, train_acc, val_acc = history\n","\n","plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.25)\n","\n","plt.subplot(121)\n","plt.plot(loss)\n","plt.xlabel('Epoch')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","\n","plt.subplot(122)\n","plt.plot(train_acc, label='train')\n","plt.plot(val_acc, label='val')\n","plt.legend()\n","plt.xlabel('Epoch')\n","plt.ylabel('Clasification accuracy')\n","plt.title('Classification accuracy history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"waoBKzJqUYMr"},"source":["---\n","##3 - Training Accuracy\n","Calculate the loss, and train-validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nSQ4AgzqUYMs","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_multi_layer(X_train, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy = %0.2f%%' %  (accuracy*100))\n","\n","print('Training label  =',y_train[:15])\n","print('Predicted label =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XsmjABg9UYMz","colab":{}},"source":["y_pred = predict_multi_layer(X_val, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy = %0.2f%%' %  (accuracy*100))\n","\n","print('Validation label =',y_val[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yDCzeeV1JtY7","colab":{}},"source":["y_pred = predict_multi_layer(X_test, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n","print('Testing Accuracy = %0.2f%%' %  (accuracy*100))\n","\n","print('Testing label    =',y_test[:15])\n","print('Predicted label  =',y_pred[:15])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vnCN6_Fp6E57"},"source":["---\n","## 4 - View Result\n","Now to visualise some of the model's predictions:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x83E9GAi6E5_","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(22,6))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","\n","for j in range(0,2):\n","    for i in range(0, 10):\n","\n","        img_index = np.random.randint(0, 10000)\n","        ax[j,i].imshow(X_test_ori[img_index])\n","\n","        actual_label    = int(y_test[img_index])\n","        predicted_label = int(y_pred[img_index])\n","\n","        color = 'red'\n","        if actual_label == predicted_label:\n","            color = 'green'\n","\n","        ax[j,i].set_title(\"Actual: {} ({})\\n Predicted: {} ({})\".format(\n","            actual_label, class_names[actual_label], predicted_label, class_names[predicted_label]\n","            ), color=color)\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation\n","\n","<font size=5> You've Completed Self-Practice 2</font>\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]}]}