{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLSS2020TU - Practical 1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMhP48TGPHTytR3CsS16NdL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SB0EeXzyu_sz","colab_type":"text"},"source":["# Practical 1: Neural Network using Tensorflow\n","\n","Â© Machine Learning Summer School - Telkom University\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN","colab_type":"text"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","\n","<a target=\"_blank\" href=\"https://colab.research.google.com/github/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Practical%201.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" height=20 alt='open in colab'/></a>\n","\n","</td><td>&nbsp;|&nbsp;\n","</td><td>\n","<a target=\"_blank\" href=\"https://github.com/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Practical%201.ipynb\"><img src=\"https://i.ibb.co/GR67738/pinpng-com-github-logo-png-945585.png\" height=20 /></a>\n","</td></tr></table>\n"]},{"cell_type":"markdown","metadata":{"id":"sjlcHwermNUS","colab_type":"text"},"source":["\n","You have written codes in self-practice to provide a whole host of vectorized neural network functionality.\n","\n","For this exercise, though, we're going to leave behind your beautiful codebase and instead migrate to one of two popular deep learning frameworks: in this instance, **TensorFlow**\n","\n","The goals of this assignment are as follows:\n","* Understand how to use **Tensorflow Eager** and **Keras Layers API** to build a neural network architecture\n","* Building Model using Sequential and Functional API\n","* Understand how a model is trained and evaluated\n","* Understand the concept of train / validation / test split and why it's useful\n","* Research at least 1 technique that can be used to improve model generalization\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"6VAE_sE-Bgk1","colab_type":"text"},"source":["---\n","---\n","#[Part 0] Preparation"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pMlC1Kwa9Dl0"},"source":["---\n","## About Tensorflow\n","\n","\n","<img src=\"https://www.gstatic.com/devrel-devsite/prod/vbf66214f2f7feed2e5d8db155bab9ace53c57c494418a1473b23972413e0f3ac/tensorflow/images/lockup.svg\" alt=\"tensorflow\" width=\"300px\"/>\n","\n","[TensorFlow (TF)](https://www.tensorflow.org/) is a **Deep Learning Library**, developed by the Google Brain Team within the Google Machine Learning Intelligence research organization, for the purposes of machine learning and artificial neural network research.\n","\n","TensorFlow is a system for executing computational graphs over Tensor objects, with native support for performing backpropagation for its Variables. In it, we work with Tensors which are n-dimensional arrays analogous to the numpy ndarray.\n","\n","\n","\n","**Tensorflow Key Features**\n","\n","* Define, optimize and efficiently calculate mathematical expressions involving multi-dimensional arrays (tensors).\n","* Programming support from deep neural networks and machine learning techniques.\n","* Use of GPU and TPU computing and automatic memory optimization.\n","* High computing capability across machines and large data sets.\n","\n","TensorFlow is available with Python and C ++ support, but the Python API is more supported and easier to learn.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zf0ePYsfVHvc","colab_type":"text"},"source":["---\n","## About Keras\n","\n","<img src=\"https://s3.amazonaws.com/keras.io/img/keras-logo-2018-large-1200.png\" alt=\"keras\" width=\"300px\"/>\n","\n","[Keras](https://keras.io/) is a very modular and minimalist **Deep Learning Library**, written in Python and capable of running on TensorFlow or Theano. This library was developed with a focus on enabling fast experiments.\n","\n","At first Keras was developed to help users to easily use Theano and Tensorflow's which at the time was very technical and complex in implementation.\n","\n","Since Tensorflow version 1.5, Keras was adopted by Google and since then the built API has been included in the Tensorflow distribution."]},{"cell_type":"markdown","metadata":{"id":"MHosKd7SVR78","colab_type":"text"},"source":["---\n","\n","Working with Tensorflow will give us benefits:\n","\n","* Our code will now run on GPUs! Much faster training. Writing your own modules to run on GPUs is beyond the scope of this class, unfortunately.\n","\n","* We want you to be ready to use one of these frameworks for your project so you can experiment more efficiently than if you were writing every feature you want to use by hand. \n","\n","* We want you to stand on the shoulders of giants! TensorFlow and PyTorch are both excellent frameworks that will make your lives a lot easier, and now that you understand their guts, you are free to use them :) \n","\n","* We want you to be exposed to the sort of deep learning code you might run into in academia or industry. "]},{"cell_type":"markdown","metadata":{"id":"QGarRj9tWulZ","colab_type":"text"},"source":["---\n","## GPU Runtime\n","Since we're going to use TensorFlow, we can utilize the GPU to accelerate the process\n","\n","For that, make sure that this Colaboratory file is set to use GPU\n","\n","* select **Runtime** in taskbar\n","* select **Change Runtime Type**\n","* choose Hardware accelerator **GPU**\n","\n","<center>\n","  \n","![gpu](https://i.ibb.co/QX3Brf0/gpu.png)\n"]},{"cell_type":"markdown","metadata":{"id":"12SMnaunBw96","colab_type":"text"},"source":["---\n","## Import Libraries\n","\n","Import required libraries"]},{"cell_type":"code","metadata":{"id":"hsZYqgngcZzY","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os, datetime\n","\n","%matplotlib inline\n","%load_ext tensorboard\n","\n","# tensorboard log\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fTMPdtW1mNVv","colab_type":"text"},"source":["---\n","# [Part 1] CIFAR-10 Dataset\n","\n","In this practical, we use the CIFAR-10 dataset consisting of  60,000 $32\\times32$ colour images in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images\n","\n","The idea is to train a classifier to identify the class value (what kind of object it is) given the image. We train and tune a model on the 50,000 training images and then evaluate how well it classifies the 10,000 test images that the model did not see during training. \n","\n","This task is an example of a supervised learning problem, where we are given both input and labels (targets) to learn from. "]},{"cell_type":"markdown","metadata":{"id":"J0WDo64UcKsi","colab_type":"text"},"source":["---\n","## 1 - Load Dataset\n","\n","Tensorflow has convenient modules for loading a number of standard datasets.\n","\n","You can see the various datasets provided on the following link:\n","* [keras](https://keras.io/datasets/)\n","* [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)\n","* [tf.dataset](https://www.tensorflow.org/datasets/datasets)\n","\n","for that, here let's download the mnist dataset from `tf.keras` as follows"]},{"cell_type":"code","metadata":{"id":"HJMZF1BZmNVw","colab_type":"code","colab":{}},"source":["(X_train_ori, y_train), (X_test_ori, y_test) = tf.keras.datasets.cifar10.load_data()\n","\n","class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6YNXtE039Dmd"},"source":["---\n","## 2 - Visualizing Data\n","\n","Each image in the dataset consists of a $32 \\times 32\\times3$ matrix. Each image is assigned a corresponding numerical label, so the image in ```X_train_ori[i]``` has its corresponding label stored in ```y_train[i]```. We also have a lookup array called ```class_names``` to associate a text description with each of the numerical labels. For example, the label 1 is associated with the text description \"Plane\".\n","\n","Show the first 20 images from `X_train`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oh3fg51s9Dmf","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","label = y_train.ravel()\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train_ori[i+j*10])\n","        ax[j,i].set_title(class_names[label[i+j*10]])\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c57WBdtqmNVz","colab_type":"text"},"source":["---\n","## 3 - Split Validation Data\n","\n","When we build machine learning models, the goal is to build a model that will perform well on *future* data that we have not seen yet. We say that we want our models to be able to **generalize** well from whatever training data we can collect and do have available, to whatever data we will be applying them to in future. \n","\n","To do this, we split whatever data we have available into a **training set, a validation set and a test set**. The idea is that we train our model and use the performance on the validation set to make any adjustments to the model and its hyperparameters, but then we report the final accuracy on the test set. \n","\n","The test set (which we never train on), therefore acts as a proxy for our future data, and **should not** be used during observation.\n","\n","Now let's remove 10,000 images and labels from the training set to use as a validation set. "]},{"cell_type":"code","metadata":{"id":"5vuvkKCdmNV1","colab_type":"code","cellView":"both","colab":{}},"source":["X_val_ori   = X_train_ori[-10000:,:]\n","y_val       = y_train[-10000:]\n","\n","X_train_ori = X_train_ori[:-10000, :]\n","y_train     = y_train[:-10000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJZisKPImNV4","colab_type":"text"},"source":["---\n","## 4 - Normalize Data\n","\n","Normalization refers to normalizing the data dimensions so that they are of approximately the same scale. There are two common ways of achieving this normalization. One is to divide each dimension by its standard deviation, once it has been zero-centered (Mean subtraction preprocessing)\n","\n","Mean subtraction is the most common form of preprocessing. It involves subtracting the mean across every individual feature in the data, and has the geometric interpretation of centering the cloud of data around the origin along every dimension"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"TwP9wbYLmNV5","colab_type":"code","cellView":"both","colab":{}},"source":["X_train = X_train_ori.astype('float32')\n","X_val   = X_val_ori.astype('float32')\n","X_test  = X_test_ori.astype('float32')\n","\n","mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n","std_pixel  = X_train.std(axis=(0, 1, 2), keepdims=True)\n","\n","X_train = (X_train - mean_pixel) / std_pixel\n","X_val   = (X_val - mean_pixel) / std_pixel\n","X_test  = (X_test - mean_pixel) / std_pixel\n","\n","# X_train /= 255\n","# X_val   /= 255\n","# X_test  /= 255\n","\n","image_shape = X_train.shape[1:]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ISmCCfEH9Dmr"},"source":["Check the implementation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pSi7HV2g9Dmu","colab":{}},"source":["print('X_train.shape =',X_train.shape)\n","print('X_val.shape   =',X_val.shape)\n","print('X_test.shape  =',X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qFYpneoiM1u","colab_type":"code","colab":{}},"source":["print('y_train.shape =',y_train.shape)\n","print('y_val.shape   =',y_val.shape)\n","print('y_test.shape  =',y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sipVd5-6id7R","colab_type":"text"},"source":["---\n","# [Part 2] Preparing Data with TensorFlow\n","At the moment, our training data consists of two large tensors. The images are stored in a tensor of shape $[40000, 32, 32, 3]$, consisting of all the $32 \\times 32 \\times 3$ images matrices stacked together. The labels are stored in a 1D vector of shape $[40000]$. \n","\n","We wish to train a model using **mini-batch stochastic gradient descent**. In order to do so, we need to shuffle the data and split it into smaller (mini-)batches. We also convert the data from numpy arrays to TensorFlow Tensors.\n","\n","In order to do this batching (and shuffling) we will use the Tensorflow [Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset), which is a set of simple reusable components that allow you to build efficient data pipelines. "]},{"cell_type":"markdown","metadata":{"id":"COaHTen8XvJo","colab_type":"text"},"source":["---\n","## 1 - Define Batch Dataset\n","\n","We start by defining the `batch_size` hyperparameter of our model. \n","> This hyperparameter controls the sizes of the mini-batches (chunks) of data that we use to train the model. The value you use will affect the memory usage, speed of convergence and potentially also the performance of the model. It also interacts with the learning rate used in gradient descent.  \n","\n","Then, we  group the image and label tensors together into a tuple and then split them into individual entries using [from_tensor_slices()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) function.\n","\n","The next thing we do is add a shuffle component and put the batch component to return a random batch of slices from the dataset. The output of this pipeline will be tuples of tensors containing images and label. The images will be of shape `(batch_size, 32, 32, 3)` and the labels of shape `(batch_size, )`"]},{"cell_type":"code","metadata":{"id":"b2-LgxEDidWV","colab_type":"code","colab":{}},"source":["# define the batch size\n","batch_size = 128\n","\n","train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n","train_ds = train_ds.shuffle(buffer_size=batch_size * 10)\n","train_ds = train_ds.batch(batch_size)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ig6GaenZalBE","colab_type":"text"},"source":["We do the same for the validation dataset, except we don't need to shuffle this time."]},{"cell_type":"code","metadata":{"id":"w77zK-kplCV-","colab_type":"code","colab":{}},"source":["val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n","val_ds = val_ds.batch(batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PHOSFVyY4e3f","colab_type":"text"},"source":["---\n","# [Part 3] Keras Model API\n","Implementing a neural network using the low-level TensorFlow API is a good way to understand how TensorFlow works, but it's a little inconvenient.\n","\n","Furthermore, since the release of version 2.0, Tensorflow is standardizing its implementation using High-level Keras API. This is enabled by the introduction of the [\"Eager Execution mode\"](https://www.tensorflow.org/guide/eager) which allows you to evaluates Tensor operations imperatively (in the order you write them), similar to NumPy and PyTorch, without building graphs.\n","\n","Eager-mode is slightly less performant but a lot more intuitive. This makes it easy to get started with TensorFlow and debug models, and it reduces boilerplate as well.\n","\n","\n","In this part of the notebook we will define neural network models using the&nbsp; `tf.keras.Sequential` &nbsp;and&nbsp; `tf.keras.Model` API. \n"]},{"cell_type":"markdown","metadata":{"id":"ZyT-JsZqWbhX","colab_type":"text"},"source":["---\n","## 1 - Keras Model and Layers\n","\n","Keras is a Deep Learning API that was built as an independent open source project by more than 700 contributors. \n","\n","During its construction until, Keras is constantly updated so that there are often changes in the technical side of writing code. One of them is how to define a network model.\n","\n","There are two basic model building in Keras, using linear [**Sequential**](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model, or using more advance graphical [**Models**](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n","\n","For now, let's import those two packages\n"]},{"cell_type":"code","metadata":{"id":"MWObKeb8xa8I","colab_type":"code","colab":{}},"source":["from tensorflow.keras import Sequential\n","from tensorflow.keras import Model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxmOJgNJcpvq","colab_type":"text"},"source":["\n","For this example, let's create a **2 layers neural network** with **100 neurons** in its hidden layer\n","\n","There are four types of layers that we will use to build this model:\n","\n","     * Input layer      to receive input shape\n","     * Flatten layer    to reshape input into one-dimensional matrix for neural network\n","     * Dense layer      to add affine fully connected layer\n","     * Activation layer to add nonlinearity\n","     \n","For other types of layers, you can look it in **[tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers)**"]},{"cell_type":"code","metadata":{"id":"kzXvQ69tcrO6","colab_type":"code","colab":{}},"source":["from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmtjFMeOw9zR","colab_type":"text"},"source":["---\n","## 2 - Old Sequential API\n","\n","The first way to build a model using Keras, and one of the oldest ways, is to initialize the [**Sequential**](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) model object, \n","\n","then one by one we add the layers that we want to stack as follows\n"]},{"cell_type":"code","metadata":{"id":"2iLF9zBoyIPb","colab_type":"code","colab":{}},"source":["model = Sequential(name='my_model_1')\n","\n","model.add( Input(image_shape) )         # input layer to receive image 32x32x3\n","model.add( Flatten() )                  # Flatten layer to reshape input into 3072x1\n","model.add( Dense(100) )                 # First affine layer (hidden layer) with 100 neurons\n","model.add( Activation('sigmoid') )      # Sigmoid activation function\n","model.add( Dense(10) )                  # Second affine layer (output layer) with 10 neuron\n","model.add( Activation('softmax') )      # Softmax activation function\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiat15JzIM5u","colab_type":"text"},"source":["---\n","## 3 - Compact Sequential API\n","\n","The update on the Keras layer API allows us to add the input shape into the first layer directly.\n","It also allows up to add the activation function directly from the `Dense` layer without adding the `Activation` layer. \n"]},{"cell_type":"code","metadata":{"id":"K3oJwZl5IGmg","colab_type":"code","colab":{}},"source":["# create model new\n","model = Sequential(name='my_model_2')\n","\n","model.add( Flatten(input_shape=image_shape) )    # Flatten layer to receive image 32x32x3 and reshape it into 3072x1\n","model.add( Dense(100, activation='sigmoid') )    # First affine layer (hidden layer) with 100 neurons and sigmoid activation\n","model.add( Dense(10,  activation='softmax') )    # Second affine layer (output layer) with 10 neuron and softmax activation\n","      \n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IZY1O_Jx_q52","colab_type":"text"},"source":["---\n","## 4 - Functional Model API\n","The third way to build models is to use **functional API** which allows us to build more complex model graphs, for example, having many input and output. The functional API can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs. After the graph is defined, group layers into an object using [**Models**](https://www.tensorflow.org/api_docs/python/tf/keras/Model) module\n","\n","The following is an example of building a model using **functional API**"]},{"cell_type":"code","metadata":{"id":"HNlMwWsI_q54","colab_type":"code","colab":{}},"source":["\n","# create model graph\n","in_node  = Input(shape=image_shape)               # define input node to receive image 32x32x3\n","x        = Flatten() (in_node)                    # define x node as Flatten layer that receive input node\n","x        = Dense(100, activation='sigmoid')(x)    # pass x node to Dense hidden layer\n","out_node = Dense(10,  activation='softmax')(x)    # define output node as Dense layer that receive x node\n","\n","# initialize the model\n","model = Model(in_node, out_node, name='my_model_3')\n","\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5BBi5KC_q56","colab_type":"text"},"source":["---\n","## 5 - Sequential API (Current)\n","The fourth way is a new way, and the current standard, of building a Sequential model. \n","\n","Similar to the first and second ways, but we can directly register the layers in the list when initializing the Sequential object as follows"]},{"cell_type":"code","metadata":{"id":"R0xzjgWQ_q56","colab_type":"code","colab":{}},"source":["# create model compact sequential\n","\n","model = Sequential([                    \n","    Flatten(input_shape=image_shape),   # Flatten layer to receive image 32x32x3 and reshape it into 3072x1\n","    Dense(100, activation='sigmoid'),   # First affine layer (hidden layer) with 100 neurons and sigmoid activation  \n","    Dense(10,  activation='softmax')    # Second affine layer (output layer) with 10 neuron and softmax activation\n","], name='my_model_4')\n","\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHN9ka7dG6bN","colab_type":"text"},"source":["---\n","# [Part 4] Tensorflow Training Function\n","\n","In graph mode, TensorFlow builds up a \"computation graph\" which captures all operations of the model and their dependencies. For training, the gradient can then be computed by traversing backwards from every node through its dependents and applying the \"chain rule\" of differentiation.\n","\n","However, in Eager mode, we don't have the concept of the computation graph anymore. Operations are performed imperatively (in the order in which they were executed). \n"]},{"cell_type":"markdown","metadata":{"id":"c8Pcv9G0Qjzc","colab_type":"text"},"source":["---\n","## 1 - Define Model\n","\n","In this section we'll build a classifier. Specifically, we will build a classifier that takes in (a batch of) $[32\\times32\\times3]$ CIFAR-10 images as we've seen above, and outputs predictions about which class the image belongs to. \n","\n","\n","> <font color='red'>**EXERCISE**: </font> Define your classification model. \n","\n","You can define the model using Sequential or Functional API. You can also experiment by trying to change the layer type, activation functions, and number of neurons. However, for this pipeline, do not add activation function to the output layer."]},{"cell_type":"code","metadata":{"id":"jxM2gPBdlV-R","colab_type":"code","colab":{}},"source":["model = Sequential([\n","                             \n","    Flatten(input_shape=image_shape),  \n","\n","    ??,   # add a dense layer with 256 neurons and relu activation\n","    ??,   # add a dense layer with 128 neurons and relu activation\n","\n","    # Create an \"output layer\" with 10 neurons, \n","    # do not add activation to this output layer\n","    Dense(10),\n","\n","], name='my_model')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5W3SaXjUc4NN","colab_type":"text"},"source":["### Activation Functions\n","\n","Activation functions are a core ingredient in deep neural networks. In fact they are what allows us to make use of multiple layers in a neural network. There are a number of different activation functions, each of which are more or less useful under different circumstances. The four activation functions that you are most likely to encounter are, arguably, [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU), [Tanh](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh), [Sigmoid](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid), and [Softmax](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax). \n","\n","More of activation functions can be seen at [keras activations](https://keras.io/api/layers/activations/) or [tf.keras activations](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Activation)"]},{"cell_type":"markdown","metadata":{"id":"mQiRgmbudo3S","colab_type":"text"},"source":["### Model Summary\n","\n","The following summary shows how many parameters each layer is made up of (the number of entries in the weight matrices and bias vectors). Note that a value of ```None``` in a particular dimension of a shape means that the shape will dynamically adapt based on the shape of the inputs."]},{"cell_type":"code","metadata":{"id":"BJW5YA0WlhqO","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3iqPeXbfdz0G","colab_type":"text"},"source":["### Plot Model\n","\n","Plot model is a utility function to converts a Keras model to dot format and save to a file. This enables us to visualize the model more easily, especially for non-linear models."]},{"cell_type":"code","metadata":{"id":"VF4cEoDbHDqC","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import plot_model\n","\n","plot_model(model, \n","           to_file=model.name+'.png', \n","           show_shapes=True, \n","           show_layer_names=False,\n","           rankdir='LR',\n","           dpi=70\n","          )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NPFnNas5T-52","colab_type":"text"},"source":["---\n","## 2 - Optimizer and Loss\n","\n","To optimize the model, we need to specify a loss function for our classifier. This tells us how good our model's predictions are compared to the actual labels (the targets), with a lower loss meaning better predictions. \n","The optimizer also responsible for controlling the learning rate.\n"]},{"cell_type":"markdown","metadata":{"id":"fwaWC62ajf2m","colab_type":"text"},"source":["### Optimizer Functions\n","\n","There are various types of optimization functions such as:\n","* `sgd` &nbsp;for standard&nbsp; `stochastic gradient descent`, including &nbsp;`nesterov`\n","* `rmsprop` &nbsp;which is a further development of 'sgd'\n","* `adam` &nbsp;as the current standard optimization function\n","* and many others\n","\n","<br>\n","\n","Here we use the **Adam optimizer** to train our neural networks. Adam is a variant of stochastic gradient descent which often performs well in practice. Here is an illustration showing how a few of these methods perform on a toy problem: \n","\n","<table>\n","  <tr><td  align=\"center\">\n","    <img src=\"https://cs231n.github.io/assets/nn3/opt1.gif\" width=\"70%\" \n","         alt=\"Optimization algorithms visualized over time in 3D space.\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure:</b> Optimization algorithms visualized over time in 3D space.<br/>(Source: <a href=\"http://cs231n.github.io/neural-networks-3/\">Stanford class CS231n</a>, MIT License, Image credit: <a href=\"https://twitter.com/alecrad\">Alec Radford</a>)\n","  </td></tr>\n","</table>"]},{"cell_type":"code","metadata":{"id":"qvzD08cj0wF1","colab_type":"code","colab":{}},"source":["# Instantiate an optimizer to train the model.\n","optimizer   = tf.keras.optimizers.Adam()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjCcAbOaivUj","colab_type":"text"},"source":["\n","More of optimizer functions can be seen at [keras optimizers](https://keras.io/api/optimizers/) or [tf.keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"]},{"cell_type":"markdown","metadata":{"id":"V4SUoegchsDz","colab_type":"text"},"source":["### Loss Functions\n","\n","there are also various types of **loss functions** for various cases such as:\n","* `categorical crossentropy` &nbsp; for multi-class classifications\n","* `binary crossentropy` &nbsp; for binary classification\n","* `mean squared error` &nbsp; for regression\n","* and many others\n","\n","<br>\n","\n","The standard loss function to use with a multi-class classifier is the **cross-entropy loss** also known as the \"negative log likelihood\" for a classifier.\n","\n","For a a classification problem with $C$ classes, the cross-entropy loss is defined as\n","\n","$$Loss = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C log( p(y|X_i)_c) \\mathbb{1}[y_i=c]$$\n","\n","\n","Fortunately we don't need to write this function ourselves as Tensorflow provides a version called \n","\n","```tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)```."]},{"cell_type":"code","metadata":{"id":"ab9tXVH4htTy","colab_type":"code","colab":{}},"source":["# Instantiate a loss function.\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3fRwTkIx0_Iv"},"source":["\n","More of loss functions can be seen at [keras losses](https://keras.io/api/losses/) or [tf.keras losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)"]},{"cell_type":"markdown","metadata":{"id":"RFuTZPYfVDxX","colab_type":"text"},"source":["### Metrics Function\n","\n","Select metrics to measure the loss and the accuracy of the model. \n","These metrics accumulate the values over epochs and then print the overall result."]},{"cell_type":"code","metadata":{"id":"AebIUnG80xke","colab_type":"code","colab":{}},"source":["from tensorflow.keras import metrics\n","\n","train_loss     = metrics.Mean(name='train_loss')\n","val_loss       = metrics.Mean(name='val_loss')\n","\n","train_accuracy = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","val_accuracy   = metrics.SparseCategoricalAccuracy(name='val_accuracy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksS-p-a-VnrC","colab_type":"text"},"source":["---\n","## 3 - Step Function\n","\n","\n","The default runtime in TensorFlow 2.0 is eager execution. TensorFlow uses a mechanism called the **\"GradientTape\"** for computing gradients in Eager mode. \n","\n","Basically, the gradient tape records the order of all operations as they are executed, and can then be \"run backwards\" (traversed from last to first operation) for computing the gradients.\n","\n","This is great for debugging, but graph compilation has a definite performance advantage. \n","\n","Decribing your computation as a static graph enables the framework to apply global performance optimizations.\n"]},{"cell_type":"markdown","metadata":{"id":"Krin326Rj27-","colab_type":"text"},"source":["### Train Step\n","Define a function to train the model using `tf.GradientTape`. This opens a GradientTape to record the operations run during the forward pass, which enables auto-differentiation. Inside the function we run the forward pass of the layer, and update the parameters using the recorded gradients of the trainable variables.\n","\n","\n","> <font color='red'>**EXERCISE**: </font> complete the function"]},{"cell_type":"code","metadata":{"id":"XGmew_G90tqA","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(x, y):\n","\n","    # Initialise a GradientTape to track the operations\n","    with tf.GradientTape() as tape:\n","\n","        # Compute the logits (un-normalised scores) of the current batch of examples\n","        # using the neural network architecture we defined earlier\n","        # run model() with input x and set training=True\n","        logits = ??\n","\n","        # Compute the loss value for this minibatch.\n","        # call loss_fn() function with input y and logits\n","        loss_value = ??\n","\n","    # Use the gradient tape to automatically retrieve\n","    # the gradients of the trainable variables with respect to the loss.\n","    gradients = tape.gradient(loss_value, model.trainable_weights)\n","\n","    # Run one step of gradient descent by updating\n","    # the value of the variables to minimize the loss.\n","    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n","\n","    # Update train metrics\n","    train_loss(loss_value)\n","    train_accuracy.update_state(y, logits)\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFegUL2ZWejk","colab_type":"text"},"source":["### Test Step\n","\n","Define a function to evaluate the model. This function will run a validation loop at the end of each epoch.\n","\n","> <font color='red'>**EXERCISE**: </font> complete the function"]},{"cell_type":"code","metadata":{"id":"N5p4OKH-0tnS","colab_type":"code","colab":{}},"source":["@tf.function\n","def test_step(x, y):\n","\n","    # run model() with input x and set training=False\n","    logits = ??\n","\n","    # call loss_fn() function with input y and logits\n","    loss_value = ??\n","\n","    # Update val metrics\n","    val_loss(loss_value)\n","    val_accuracy.update_state(y, logits)\n","\n","    return logits, loss_value, val_accuracy.result()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6da4wNnXAGw","colab_type":"text"},"source":["---\n","## 4 - Training Loop\n","\n","Define the training loop as follow:\n","* Define a `for` loop that iterates over epochs. For each epoch:\n","    * open a `for` loop that iterates over the training dataset, in batches.\n","      <br>for each train batch, call the `train_step()` function\n","    * open another `for` loop that iterates over the validation dataset, in batches.\n","      <br>for each validation batch, call the `test_step()` function\n","\n","> <font color='red'>**EXERCISE**: </font> complete the function"]},{"cell_type":"code","metadata":{"id":"WJCxE44olhnr","colab_type":"code","colab":{}},"source":["# The number of epochs to run\n","num_epochs = 20  \n","\n","# Lists to store the loss and accuracy of every epoch\n","history = {}\n","history['loss']     = []\n","history['val_loss'] = []\n","history['acc']      = []\n","history['val_acc']  = []\n","\n","for epoch in range(num_epochs):\n","\n","    # Loop over our data pipeline\n","    for x_batch_train, y_batch_train in train_ds:\n","        # call train_step() function with input x_batch_train and y_batch_train\n","        ??\n","        \n","    # Run a validation loop at the end of each epoch.\n","    for x_batch_val, y_batch_val in val_ds:\n","        # call test_step() function with input x_batch_val and y_batch_val\n","        ??\n","    \n","    # add current loss and accuracy to history\n","    history['loss'].append(train_loss.result())\n","    history['val_loss'].append(val_loss.result())\n","\n","    history['acc'].append(train_accuracy.result())\n","    history['val_acc'].append(val_accuracy.result())\n","\n","    # print current loss and accuracy\n","    template = 'Epoch {:03d}, Loss: {:.4f}, Train Acc: {:.2%}, Val Acc: {:.2%}'\n","    print(template.format(epoch+1,\n","                          train_loss.result(), \n","                          train_accuracy.result(),\n","                          val_accuracy.result()))\n","    \n","\n","    # Reset training metrics at the end of each epoch\n","    train_accuracy.reset_states()\n","    val_accuracy.reset_states()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vWg1Ojmcl4rU","colab_type":"text"},"source":["The code block shows a typical training loop. There's actually an easier way to do this using Tensorflow and Keras, which we'll see later."]},{"cell_type":"markdown","metadata":{"id":"Yc8w-6vLXXBy","colab_type":"text"},"source":["---\n","## 5 - Visualize Training History\n","Plot the loss and accuracy during training"]},{"cell_type":"code","metadata":{"id":"B4q0inm0lhkv","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","# Plot training & validation accuracy values\n","plt.plot(history['acc'])\n","plt.plot(history['val_acc'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Val'])\n","\n","plt.subplot(122)\n","# Plot training loss values\n","plt.plot(history['loss'])\n","plt.plot(history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Val'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p-0TIyW8p1e5","colab_type":"text"},"source":["You should see that if we only use ordinary Artificial Neural Networks, overfitting occurs during training.\n","\n","You can improve this using more advanced architecture such as Convolutional Neural Network, which you will learn later. \n","\n","You can also improve the model by adding more advanced optimization scheme such as adding Dropout Layer, Batch Normalization Layer, Regularizers, and so on.\n","\n","> For Dropout, see [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) and [research paper](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) \n","\n","> For Batch Normalization, see [Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) and [research paper](http://proceedings.mlr.press/v37/ioffe15.pdf)"]},{"cell_type":"markdown","metadata":{"id":"FPKXANTwmaih","colab_type":"text"},"source":["---\n","## 6 - Evaluate Model\n","\n","Now let's test the model to the unseen dataset.\n","\n","First, convert testing dataset and its targets into tensor"]},{"cell_type":"code","metadata":{"id":"elV2XGeGnBRJ","colab_type":"code","colab":{}},"source":["tf_X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n","tf_y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tKUzSXMwxr-7","colab_type":"text"},"source":["Then call the test_step function, and get the output tuple containing logits, loss, and accuracy\n","\n","we should get accuracy around `48%`"]},{"cell_type":"code","metadata":{"id":"JBtjSaU5nnq7","colab_type":"code","colab":{}},"source":["test_logits, test_loss, test_accuracy = test_step(tf_X_test, tf_y_test)\n","\n","# get the predicted class\n","y_pred = tf.argmax(test_logits, axis=1, output_type=tf.int32)\n","\n","print('Completed testing on', tf_X_test.shape[0], 'examples...')\n","print('Loss: {:.3f}, Accuracy: {:.3%}'.format(test_loss, test_accuracy))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jizSM5bzxYFV","colab_type":"text"},"source":["---\n","## 7 - View Result\n","Now to visualize some of the model's predictions:"]},{"cell_type":"code","metadata":{"id":"d2Vzno7PnyZb","colab_type":"code","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(22,6))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","label = y_test.ravel()\n","for j in range(0,2):\n","    for i in range(0, 10):\n","\n","        img_index = np.random.randint(0, 10000)\n","        ax[j,i].imshow(X_test_ori[img_index])\n","\n","        actual_label    = int(y_test[img_index])\n","        predicted_label = int(y_pred[img_index])\n","\n","        color = 'red'\n","        if actual_label == predicted_label:\n","            color = 'green'\n","\n","        ax[j,i].set_title(\"Actual: {} ({})\\n Predicted: {} ({})\".format(\n","            actual_label, class_names[actual_label], predicted_label, class_names[predicted_label]\n","            ), color=color)\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WWD1cqo7zgEM","colab_type":"text"},"source":["---\n","# [Part 5] Keras API Training Function"]},{"cell_type":"markdown","metadata":{"id":"rQohd54TIn1x","colab_type":"text"},"source":["---\n","## 1 - One Hot Matrix\n","\n","When using Keras for multiclass classification, the first step that we have to do is convert the target into what is known as one-hot matrix. \n","\n","It change the target label into a sparse matrix with size of class number, with one in the index of the label and zeros in everywhere else\n","\n","With Keras, we can use **to_categorical** functions from **tf.keras.utils** module"]},{"cell_type":"code","metadata":{"id":"neQU8KqDIn1_","colab_type":"code","colab":{}},"source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train_hot = to_categorical(y_train, 10)\n","y_val_hot   = to_categorical(y_val, 10)\n","y_test_hot  = to_categorical(y_test, 10)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_xTz69xIn2G","colab_type":"text"},"source":["example of 10 first one-hot label from training data"]},{"cell_type":"code","metadata":{"id":"Gl6iGzGgIn2I","colab_type":"code","colab":{}},"source":["print('  |   |  class:\\ni | y |  0 1 2 3 4 5 6 7 8 9\\n---------------------------------')\n","for i in range(10):\n","    print(i, '|', y_train[i], '|', y_train_hot[i,:].astype('int'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GykTPRsYT8Xd","colab_type":"text"},"source":["---\n","## 2 - Define Model\n","\n","Same as before, we define the classifier model that takes in (a batch of) $[32\\times32\\times3]$ CIFAR-10 images as we've seen above, and outputs predictions about which class the image belongs to. \n","\n","\n","> <font color='red'>**EXERCISE**: </font> Define your classification model. \n","\n","You can define the model using Sequential or Functional API. You can also experiment by trying to change the layer type, activation functions, and number of neurons. \n","\n","For this model, use `softmax` activation for the output layer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MxneSJ-A00bJ","colab":{}},"source":["model = Sequential([\n","                             \n","    Flatten(input_shape=image_shape),  \n","\n","    ??,   # add a dense layer with 256 neurons and relu activation\n","    ??,   # add a dense layer with 128 neurons and relu activation\n","    ??,   # add a dense layer with 10 neurons and softmax activation\n","\n","], name='my_model')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9xtgVaMUrfK","colab_type":"text"},"source":["Show the model summary"]},{"cell_type":"code","metadata":{"id":"KLES7pel0pSg","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1VwdNli3yHlA","colab_type":"text"},"source":["---\n","## 3 - Compile Model\n","\n","When using the Keras API, to register loss functions and its optimization function, we use `.compile()` method. Conveniently, we can choose the loss, optimizer, and metrics using their string identifier."]},{"cell_type":"code","metadata":{"id":"V94Cyio-yOAH","colab_type":"code","colab":{}},"source":["# Compile model\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iaBCxATWVyMV","colab_type":"text"},"source":["To see the string identifier for loss functions, see [keras losses](https://keras.io/api/losses/) or [tf.keras losses](https://www.tensorflow.org/api_docs/python/tf/keras/losses)\n","\n","To see the string identifier for optimizers, see [keras optimizers](https://keras.io/api/optimizers/) or [tf.keras optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"]},{"cell_type":"markdown","metadata":{"id":"YoOA7l_eZDCo","colab_type":"text"},"source":["---\n","## 4 - Train Model\n","\n","Now we can train the model by calling the &nbsp; `.fit()` &nbsp; method.\n","\n","Run the training process for epoch=5 with batch size=100"]},{"cell_type":"code","metadata":{"id":"ZjxJaQ96yNvc","colab_type":"code","colab":{}},"source":["num_epochs = 20\n","batch_size = 128\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","history = model.fit(X_train, y_train_hot, \n","                    validation_data=(X_val, y_val_hot),\n","                    epochs=num_epochs, \n","                    batch_size=batch_size, \n","                    callbacks = [tensorboard_callback],\n","                    verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UyaM5xaZerqS"},"source":["you can further train the model simply by re-run the cell "]},{"cell_type":"markdown","metadata":{"id":"vJ0U_NkcUZ9z","colab_type":"text"},"source":["---\n","## 5 - Visualize Training History\n","\n","Visualize the train-validation accuracy"]},{"cell_type":"code","metadata":{"id":"QlJpKQBKUjRV","colab_type":"code","colab":{}},"source":["plt.rcParams['figure.figsize'] = [14, 3.5]\n","plt.subplots_adjust(wspace=0.2)\n","\n","plt.subplot(121)\n","# Plot training & validation accuracy values\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'])\n","\n","plt.subplot(122)\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oVwhXzLieo-q","colab_type":"text"},"source":["We can also visualize the training to Tensorboard"]},{"cell_type":"code","metadata":{"id":"vQYBrVlme09T","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ibLtwIAIn5V","colab_type":"text"},"source":["---\n","## 6 - Evaluate Model\n","Next, let's evaluate the accuracy of the models that have been trained\n","\n","we should get accuracy around `48%`"]},{"cell_type":"code","metadata":{"id":"VTD6WFU2In5W","colab_type":"code","colab":{}},"source":["scores = model.evaluate(X_test, y_test_hot, verbose=1)\n","print(\"\\nModel Accuracy: %.2f%%\" % (scores[1]*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vnCN6_Fp6E57"},"source":["---\n","## 7 - View Result\n","Now to visualize some of the model's predictions:"]},{"cell_type":"code","metadata":{"id":"OlalGMe16168","colab_type":"code","colab":{}},"source":["y_pred = model.predict(X_test, verbose=0)\n","y_pred = np.argmax(y_pred, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"x83E9GAi6E5_","colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(22,6))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","\n","for j in range(0,2):\n","    for i in range(0, 10):\n","\n","        img_index = np.random.randint(0, 10000)\n","        ax[j,i].imshow(X_test_ori[img_index])\n","\n","        actual_label    = int(y_test[img_index])\n","        predicted_label = int(y_pred[img_index])\n","\n","        color = 'red'\n","        if actual_label == predicted_label:\n","            color = 'green'\n","\n","        ax[j,i].set_title(\"Actual: {} ({})\\n Predicted: {} ({})\".format(\n","            actual_label, class_names[actual_label], predicted_label, class_names[predicted_label]\n","            ), color=color)\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation\n","\n","<font size=5> You've Completed Practical 1</font>\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]}]}