{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLSS2020TU - Self Practice 1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPmqqbsRkLsWta9ND8jEu0r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SB0EeXzyu_sz","colab_type":"text"},"source":["# Self Practice 1: Single Layer Perceptron using Numpy\n","\n","Â© Machine Learning Summer School - Telkom University\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"oUkeuLWzoHoN","colab_type":"text"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%201/MLSS2020TU%20-%20Self%20Practice%201.ipynb)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fqiFD42_92oE"},"source":["\n","In this exercise you will practice putting together a simple image classification pipeline, based on the Two-layer Neural Network classifier. The goals of this assignment are as follows:\n","\n","1. understand the basic Image Classification pipeline using Multi-layered Neural Networkhyperparameter tuning.\n","1. implement and apply standard artificial neuron function\n","1. implement and apply various classic activation functions\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"vXUed3x_ROCm","colab_type":"text"},"source":["\n","# [Part 0] Preparation\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wwfchKW2tnuH","colab_type":"text"},"source":["## 1 - Import Libraries"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JLQflrey92oM","colab":{}},"source":["import numpy as np\n","\n","from PIL import Image\n","import cv2 as cv\n","import h5py    \n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","np.set_printoptions(precision=7)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K86RgAJltrKE","colab_type":"text"},"source":["## 2 - Helper Functions"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1PAp0FCP92o5","colab":{}},"source":["def load_dataset():\n","    !wget -q 'https://raw.githubusercontent.com/CNN-ADF/Task2019/master/resources/catvnoncat.h5'\n","    dataset = h5py.File('catvnoncat.h5', \"r\")\n","    print('dataset downloaded')\n","\n","    train_set_x_orig = np.array(dataset[\"X_train\"][:]) # your train set features\n","    train_set_y_orig = np.array(dataset[\"y_train\"][:]) # your train set labels\n","    val_set_x_orig = np.array(dataset[\"X_val\"][:]) # your val set features\n","    val_set_y_orig = np.array(dataset[\"y_val\"][:]) # your val set labels\n","    classes = np.array(dataset[\"classes\"][:]) # the list of classes\n","    \n","    return train_set_x_orig, train_set_y_orig, val_set_x_orig, val_set_y_orig, classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"StGPs1-YSAPq","colab_type":"code","colab":{}},"source":["def open_image(filename, show=True):\n","    image = Image.open(filename)\n","    image = np.array(image)\n","    if show:\n","        plt.imshow(image)\n","        plt.show\n","    return image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhCZUsepSRXM","colab_type":"code","colab":{}},"source":["def resize_image(image, shape=(64,64), show=True):\n","    new_image = cv.resize(image, shape, interpolation=cv.INTER_AREA)\n","    if show:\n","        plt.imshow(new_image)\n","        plt.show()\n","    return new_image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oPQaktKa92oy"},"source":["# [Part 1] Binary Classification Problem\n","\n","For this exercise, we will use binary class data to recognize `cats` and `not cats`. The images are $64\\times64$ in dimension.\n","\n","Since this is a binary classification problem, so we can use the **binary cross-entropy** loss function to measure how good the model's predictions are.\n","For that we use the **sigmoid** activation function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H-w7dUV-92o0"},"source":["## 1 - Load And Normalize Dataset\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"c_mXeRo_92o2"},"source":["\n","first, load the dataset\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fliDuquz92o-","colab":{}},"source":["X_train_ori, y_train, X_val_ori, y_val, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"upNTFz1Dqx0T","colab_type":"text"},"source":["Now let's take a look at the data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KzllPE1-92pK","colab":{}},"source":["print(X_train_ori.shape)\n","print(y_train.shape)\n","print(X_val_ori.shape)\n","print(y_val.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FTP4J31iBS9G"},"source":["View some data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xF0d5gc092pf","scrolled":true,"colab":{}},"source":["fig, ax = plt.subplots(2,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train_ori[i+j*10])\n","        ax[j,i].set_title(classes[y_train[i+j*10,0]].decode(\"utf-8\") )\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqIf_DT2Rd8h","colab_type":"text"},"source":["then reshape the original data into 1-dimensional matrix, and store them as  `X_train` and `X_val`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TBZDD3K392pt","colab":{}},"source":["X_train = X_train_ori.reshape(X_train_ori.shape[0], -1)\n","X_val = X_val_ori.reshape(X_val_ori.shape[0], -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3gDM9TnZ92qE"},"source":["lastly, since for this dataset we're using sigmoid, standarize the dataset into a `range of 0-1` by dividing it with `255`"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n1vwifEp92qM","colab":{}},"source":["X_train = X_train/ 255.\n","X_val = X_val/255."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UV6I5RpY92qa"},"source":["---\n","## 2 - Artificial Neurons\n","\n","Standard Artificial Neuron is basically a linear transformation function that receive vector input `x` and weight matrix `W`, and usually followed with a bias `b`. The output of an Artificial Neuron is defined as follow:\n","\n","$$\n","\\begin{align}\n","f(x, W, b) = x.W + b\n","\\end{align}\n","$$\n","\n","\n","> in Mathematics, this function also known as Affine Function\n","\n","\n","As you know, the purpose of machine learning is to automatically adjust the values of weight matrices to find a suitable decision boundary.\n","\n","This is where optimisation by **Gradient Descent** through **Backpropagation** comes in. The idea is that for each (batch of) data points, we compute the loss using the current values of the weights on the data. We then compute the gradient (or derivative) of the loss function at the current weight values.\n","\n","For that we need to define the backward (derivative) function for the Artificial Neuron as follow:\n","\n","$$\n","\\begin{align*}\n","\\partial W & = x^T.\\partial out \\\\\n","\\partial b & = \\sum \\partial out \\\\\n","\\partial x & = \\partial out.W^T \\\\\n","\\end{align*}\n","$$\n","\n","Now we're going to implement those functions"]},{"cell_type":"markdown","metadata":{"id":"F6UJH2vRRga3","colab_type":"text"},"source":["---\n","### a. Forward Affine Function\n","\n","\n","> <font color='red'>**EXERCISE**: </font> implement forward pass for Affine layer\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wUZykTok92qd","colab":{}},"source":["def affine_forward(x, W, b):  \n","  \n","    v = ??            # x dot W + b  \n","    \n","    return v"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aJak83yX92qj"},"source":["---\n","### b. Backward Affine Function\n","\n","\n","> <font color='red'>**EXERCISE**: </font> implement backward pass for Affine layer\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k2Fnd_Sm92qn","colab":{}},"source":["def affine_backward(dout, x, W, b):\n","  \n","    dW = ??           # x.T dot dout\n","    db = ??           # sum dout, axis=0, keepdims=True\n","    dx = ??           # dout dot W.T\n","    \n","    return dW, db, dx"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ipcEIrjgjbV0","colab_type":"text"},"source":["---\n","## 3 - Binary Cross-Entropy\n","\n","\n","For this classification problem, we consider the **binary cross-entropy** loss function to measure how good the model's predictions are.  This loss function compares the model's prediction for each example, $\\mathbf{x_i}$ to the true **target** $y_i$ (we often refer to the true label associated with an input as the \"target\"). It then applies the non-linear log function to penalise the model for being further from the true class.\n"]},{"cell_type":"markdown","metadata":{"id":"BZjjZYLLk4VG","colab_type":"text"},"source":["The equation for the binary cross entropy loss, on a dataset with $N$ points is defined as follows:\n","\n","\\begin{align}\n","l(\\mathbf{w}; \\mathbf{\\hat{y}}, \\mathbf{y}) = -\\frac{1}{N}\\sum_{i=1}^N y_i log(\\hat{y}_i) + (1-y_i)log(1-\\hat{y}_i)\n","\\end{align}\n","\n","where $\\hat{y}_i = \\operatorname{sigmoid}(\\mathbf{w}^T\\mathbf{x_i})$\n","\n"]},{"cell_type":"code","metadata":{"id":"BuzOAzc9NpT7","colab_type":"code","colab":{}},"source":["def binary_cross_entropy_loss(predictions, targets, epsilon=+1e-10):\n","\n","    # Compute the local loss term\n","    N = predictions.shape[0]\n","\n","    # We add 1e-10 to make the log operations numerically stable (i.e. avoid taking the log of 0.)\n","    log_likelihood = targets * np.log(predictions+epsilon) + (1.-targets)*np.log(1.-predictions+epsilon)\n","    loss = -np.sum(log_likelihood)/N\n","    \n","    return loss\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Sz-H5feRj9U","colab_type":"text"},"source":["---\n","## 4 - Sigmoid Function\n","\n","The binary cross entropy function uses an operation called a  $\\operatorname{sigmoid}$ function. This functions allows our classifier to output any real value. The binary cross entropy loss function, however, expects the predictions made by a classifier to be between $0$ and $1$. The sigmoid function \"squashes\" any real number inputs to lie in the interval $(0, 1)$.\n","\n","The sigmoid function itself is defined as:\n","\n","$$\n","\\begin{align}\n","f(x) = \\sigma(x) = \\frac{1}{1+e^{-v}}\n","\\end{align}\n","$$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i9ghf2tn92qr"},"source":["### a. Forward Sigmoid Function\n","\n","> <font color='red'>**EXERCISE**: </font>implement forward pass for sigmoid activation function\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NGRKY2nw92qr","colab":{}},"source":["def sigmoid_forward(x):  \n","  \n","    out = ??  # 1 / ( 1 + exp (-x) )\n","\n","    return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AA897jo6ZFv6"},"source":["---\n","## 5 - One-Layer Sigmoid\n","\n","We'll train a Single Layer Perceptron with Sigmoid for binary Classification using Full batch **Gradient Descent**\n"]},{"cell_type":"markdown","metadata":{"id":"_DJOVwlSRtqG","colab_type":"text"},"source":["\n","### a. Training Function\n","> <font color='red'>**EXERCISE:** </font> Implement Training Function\n","\n","* call affine forward function\n","* call sigmoid forward function\n","* call binary cross-entropy loss function\n","* call affine backward function\n","* implement weight update\n","* The network architecture should be: <br>\n","><pre><font color=\"blue\"><b>Input - FC layer - Sigmoid</b></font></pre>\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-qVsxC3w92rj","colab":{}},"source":["def train_one_layer(X, y, W=None, b=None, lr=0.005, epochs=100, verbose=True):\n","        \n","    num_train, dim = X.shape\n","    \n","    num_classes = 1\n","    \n","    # initialize weights if not provided\n","    if W is None:\n","        W = 0.02*np.random.rand(dim, num_classes)\n","    if b is None:\n","        b = np.zeros((1, num_classes))\n","\n","    # Run stochastic gradient descent to optimize W\n","    loss_history = []\n","                     \n","    for ep in range(epochs):\n","        \n","\n","        # calculate 1st layer score by calling affine forward function using X, W, and b as input\n","        layer1 = ??\n","        \n","        # calculate 1st activation score by calling sigmoid forward function using layer1 score as input\n","        act1 = ??\n","        \n","        # calculate error by subtracting act1 with y\n","        error = ??\n","        \n","        # calculate loss by calling binary cross entropy loss function using act1 and y as input \n","        loss = ??\n","        \n","        # divide error by num_train    \n","        error /= num_train\n","    \n","        # calculate layer 1 weights gradient by calling affine backward function using error, X, W, and b as input\n","        dW, db, _ = ??\n","\n","        \n","        # perform parameter update by subtracting each W and b with a fraction of dW and db\n","        # according to the learning rate\n","        W -= lr * dW\n","        b -= lr * db\n","\n","        if verbose and ep % 100 == 0:\n","            print ('epoch',ep,'/',epochs, ': loss =', loss)\n","            \n","            # append the loss history\n","            loss_history.append(loss)\n","\n","    print('Training done')\n","    \n","    return W, b, loss_history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ye8c9cZH92rn"},"source":["### b. Train the Binary Classifier\n","\n","Try the training Function using the initial parameter"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k6LGU9zl92rn","colab":{}},"source":["W, b, loss = train_one_layer(X_train, y_train, epochs=1000, lr=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7x7U1LUT92rr"},"source":["Visualize the loss"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eppOYFhA92rr","colab":{}},"source":["plt.rcParams['figure.figsize'] = [12, 5]\n","plt.plot(loss)\n","plt.xlabel('iteration')\n","plt.ylabel('training loss')\n","plt.title('Training Loss history')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IR9hWMFBR6Sr","colab_type":"text"},"source":["\n","### c. Predict Function\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aB-Mcp8g92rw","colab":{}},"source":["def predict_one_layer(X, W, b):    \n","    y_pred = np.zeros(X.shape[1])\n","    \n","    # calculate 1st layer score by calling affine forward function using X, W, and b\n","    layer1 = affine_forward(X, W, b)\n","\n","    # calculate 1st activation score by calling sigmoid forward function using layer1 score\n","    act1 = sigmoid_forward(layer1)\n","    \n","    # since it's a binary class, round the score to get the class\n","    y_pred = np.round(act1)\n","    \n","    return y_pred.astype('int')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QJdIRG2G92ry"},"source":["### d. Training Accuracy\n","Calculate the Training Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"e8psteqc92rz","colab":{}},"source":["import sklearn\n","from sklearn.metrics import accuracy_score\n","\n","y_pred = predict_one_layer(X_train, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_train, y_pred)\n","\n","print('Training Accuracy =',accuracy*100,'%')\n","\n","print('Training label  =',y_train[:15].ravel())\n","print('Predicted label =',y_pred[:15].astype('int').ravel())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2yzbf36o92r4"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should get about <b>~97%</b> accuracy on training set using the initial run</pre>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ei9VFwzL92r5"},"source":["Calculate the Validation Accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bxCkHhzp92r6","colab":{}},"source":["y_pred = predict_one_layer(X_val, W, b)\n","accuracy = sklearn.metrics.accuracy_score(y_val, y_pred)\n","print('Validation Accuracy =', accuracy*100,'%')\n","\n","print('Validation label =',y_val[:15].ravel())\n","print('Predicted label  =',y_pred[:15].astype('int').ravel())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MMh1rxBj92r9"},"source":["**EXPECTED OUTPUT**:\n","\n","<pre>You should get about <b>~72%</b> accuracy on validation set</pre>\n","\n","<br>\n","\n","You can continue further training the weights by adding the pre-trained W and b to the arguments when calling training function\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gDDBQVUgM_OT","colab":{}},"source":["# loss, W, b = train_one_layer(X_train, y_train, W=W, b=b, num_iters=1000, learning_rate=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRN05AD1Zcxy","colab_type":"text"},"source":["---\n","## 6 - Test on New Image\n","\n","Lastly, we can use the trained weigths to try and classify images from the Internet"]},{"cell_type":"markdown","metadata":{"id":"gxfz7pGFGa1o","colab_type":"text"},"source":["### a. Load Image\n","\n","First, we download some images from the Internet"]},{"cell_type":"code","metadata":{"id":"CkFlqQoIF0YD","colab_type":"code","colab":{}},"source":["!wget -q -O 'cat.jpg'    'https://p0.pikist.com/photos/593/949/domestic-cat-head-animal-portrait-attention-close-up-cat-cat-face.jpg'\n","!wget -q -O 'no_cat.jpg' 'https://cutewallpaper.org/21/cute-wallpapers-dogs/Dog-Wallpapers-Pictures-Cute-Dogs-by-Fexy-Apps.jpg'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PDUjdNaxbuto","colab_type":"text"},"source":["let's view the images"]},{"cell_type":"code","metadata":{"id":"OBaJnd2gF6X_","colab_type":"code","colab":{}},"source":["im1 = open_image('cat.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_IqHzcewSJj5","colab_type":"code","colab":{}},"source":["im2 = open_image('no_cat.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V9i3hOXSGrn_","colab_type":"text"},"source":["### b. Resize and Normalize Image\n","\n","Since we trained the weights using normalized $64\\times64$ images, it means that we need to also resize and normalize the new images"]},{"cell_type":"code","metadata":{"id":"epsUsupMF8km","colab_type":"code","colab":{}},"source":["im1 = resize_image(im1)\n","im2 = resize_image(im2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHMGsB6oTa9g","colab_type":"code","colab":{}},"source":["im1_n = im1/255.\n","im2_n = im2/255.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FkRPK7PGGu6n","colab_type":"text"},"source":["### c. View Prediction\n","\n","Now let's feed the new data into the model, and show the predicted label"]},{"cell_type":"code","metadata":{"id":"3KxFtIOSF_fS","colab_type":"code","colab":{}},"source":["new_data = np.array([im1_n,im2_n]).reshape(2,-1)\n","\n","y_pred = predict_one_layer(new_data, W, b)\n","\n","print('predicted class:')\n","print(y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oO-WRkzWYAqj","colab_type":"code","colab":{}},"source":["print('predicted label:')\n","print(classes[y_pred])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation\n","\n","<font size=5> You've Completed Self Practice 1</font>\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]}]}