{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"MLSS2020TU - Practical 5.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SB0EeXzyu_sz"},"source":["\n","\n","# Practical 5: Variational AutoEncoder\n","\n","Â© Machine Learning Summer School - Telkom University\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oUkeuLWzoHoN"},"source":["<table  class=\"tfo-notebook-buttons\" align=\"left\"><tr><td>\n","    \n","<a href=\"https://colab.research.google.com/github/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%205/MLSS2020TU%20-%20Practical%205.ipynb\" source=\"blank\" ><img src=\"https://colab.research.google.com/assets/colab-badge.svg\"></a>\n","    \n","</td><td>\n","<a href=\"https://github.com/adf-telkomuniv/MLSS2020_Telkom/blob/master/practical%205/MLSS2020TU%20-%20Practical%205.ipynb\" source=\"blank\" ><img src=\"https://i.ibb.co/6NxqGSF/pinpng-com-github-logo-png-small.png\"></a>\n","    \n","</td></tr></table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"bqXqlfdqCzgf"},"source":["\n","An **Autoencoder** is a neural network which is an unsupervised learning algorithm which uses back propagation to generate output value which is almost close to the input value. It takes input such as image or vector anything with a very high dimensionality and run through the neural network and tries to compress the data into a smaller representation.\n","\n","<p align='center'>\n","<img src='https://i.ibb.co/cb3dQLv/ae.png' width=80% />\n","\n","\n","While the basic idea behind a **Variational Autoencoder** is that instead of mapping an input to fixed vector, input is mapped to a distribution. The only difference between the autoencoder and variational autoencoder is that bottleneck vector is replaced with two different vectors one representing the mean of the distribution and the other representing the standard deviation of the distribution.\n","\n","<p align='center'>\n","<img src=\"https://i.ibb.co/Sv2Tx2R/vae2.png\" width=80% />\n","\n","\n","In this notebook we will examine the difference between Vanilla (Convolutional) AutoEncoder and Variational AutoEncoder in generating MNIST images"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6VAE_sE-Bgk1"},"source":["---\n","---\n","#[Part 0] Import Libraries and Load Data"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WvTLgxWsfaJm"},"source":["---\n","## 1 - Import Library"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cP16VLL2cb6o","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","\n","from tensorflow.keras.layers import Lambda, Input, Dense\n","from tensorflow.keras.layers import Conv2D, Flatten\n","from tensorflow.keras.layers import Reshape, Conv2DTranspose\n","\n","from tensorflow.keras import datasets\n","from tensorflow.keras.losses import mse, binary_crossentropy\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras import backend as K\n","\n","from IPython.display import Image\n","\n","\n","import numpy as np\n","import imageio, glob, os, datetime\n","import matplotlib.pyplot as plt\n","np.set_printoptions(precision=4)\n","\n","%matplotlib inline\n","%load_ext tensorboard\n","\n","tf.random.set_seed(13)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Oh_ZsU26gPiF"},"source":["---\n","## 2 - Load MNIST Dataset\n","\n","We will work with MNIST dataset this time. However, if you ever feel bored using it, you can use the other MNIST-like datasets such as the Clothing [Fashion-MNIST](https://www.tensorflow.org/datasets/catalog/fashion_mnist), Hiragana [Kuzushiji-MNIST](https://www.tensorflow.org/datasets/catalog/kmnist), or the Mini-SketchRNN data that we've prepared."]},{"cell_type":"markdown","metadata":{"id":"08_F4fwhSVM6","colab_type":"text"},"source":["Uncomment to use MNIST Dataset"]},{"cell_type":"code","metadata":{"id":"a2I3qU4nuClT","colab_type":"code","colab":{}},"source":["(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()\n","labels = range(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SyF3oBWESfPy","colab_type":"text"},"source":["Uncomment to use Fashion-MNIST Dataset"]},{"cell_type":"code","metadata":{"id":"C1qaPVe_uClO","colab_type":"code","colab":{}},"source":["# (X_train, y_train), (X_test, y_test) = datasets.fashion_mnist.load_data()\n","# labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmC1XygDSb_u","colab_type":"text"},"source":["Uncomment to use Mini-Sketch Dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7pTsrU40ytyF","colab":{}},"source":["# !wget -q 'https://raw.githubusercontent.com/adf-telkomuniv/MLSS2020_Telkom/master/recources/mini-sketch.npy'\n","# (X_train, y_train), (X_test, y_test), labels = np.load('mini-sketch.npy', allow_pickle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V1qfBKFoTM76","colab_type":"text"},"source":["Reshape and Normalize Data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"obPzmoTKcdbP","colab":{}},"source":["image_size = X_train.shape[1]\n","\n","X_train = np.reshape(X_train, [-1, image_size, image_size, 1])\n","X_test  = np.reshape(X_test, [-1, image_size, image_size, 1])\n","\n","X_train = X_train.astype('float32') / 255\n","X_test  = X_test.astype('float32') / 255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPhN6jVDTRlK","colab_type":"text"},"source":["Visualize the first 20 images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"enDo65mm0pfL","colab":{}},"source":["print('Labels:',labels)\n","\n","fig, ax = plt.subplots(2,10,figsize=(18,5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for j in range(0,2):\n","    for i in range(0, 10):\n","        ax[j,i].imshow(X_train[i+j*10].reshape(28, 28), cmap='gray')\n","        ax[j,i].set_title(labels[y_train[i+j*10]] )\n","        ax[j,i].axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J1wtm6NLfdy4"},"source":["---\n","## 3 - Helper Functions\n","\n","Below are several helper functions to visualize the generated image"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lsoDxyCxUEKH"},"source":["---\n","### a. Plot Latent Space\n","\n","This function visualize the latent space distribution extracted from `encoder` model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2M-Ut2-ico2r","colab":{}},"source":["def plot_latent_space(encoder_model, data, batch_size=128, vae=False):\n","\n","    x, y = data\n","    if vae:\n","        z, _, _ = encoder_model.predict(x, batch_size=batch_size)\n","    else:\n","        z       = encoder_model.predict(x, batch_size=batch_size)\n","\n","    print('z range: ('+str(np.min(z))+','+str(np.max(z))+')')\n","    \n","    plt.figure(figsize=(12, 10))\n","    plt.scatter(z[:, 0], z[:, 1], c=y)\n","    plt.colorbar()\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fwuZMDU2UBVw"},"source":["---\n","### b. Generate Image\n","\n","This function generate &nbsp;$ n $ &nbsp;images from random latent space &nbsp;$ z $  &nbsp;. \n","\n","The default range of &nbsp;$ z $  &nbsp; is $(-1..1)$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BSxQxNC8fbi6","colab":{}},"source":["def generate_image(decoder_model, z_range=(-1,1), n=5):\n","    a,b = z_range\n","    z_sample = np.random.uniform(a, b, (n, 2))\n","    X_decoded = decoder_model.predict(z_sample)\n","\n","    fig, ax = plt.subplots(1,n,figsize=(15,4.5))\n","    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","    for i in range(n):\n","        digit = X_decoded[i].reshape(28, 28)\n","        ax[i].imshow(digit, cmap='gray')\n","        ax[i].set_title(str(z_sample[i]))\n","        ax[i].axis('off')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_Jf6uYK8UJxE"},"source":["---\n","### c. Plot Interpolating Images\n","This function generate &nbsp;$n\\times n$&nbsp; interpolating images generated from latent space &nbsp;$z$&nbsp;\n","\n","The default range of &nbsp;$ z $  &nbsp; is $(-1..1)$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"it9d_KXYco2z","colab":{}},"source":["def plot_interpolating(decoder_model, z_range=(-1,1), n=20, \n","                       save=False, filename='img.png', \n","                       figsize=(10, 10)):\n","\n","    # display a 30x30 2D manifold of digits\n","    a,b = z_range\n","    digit_size = 28\n","    figure = np.zeros((digit_size * n, digit_size * n))\n","\n","    # linearly spaced coordinates corresponding to the 2D plot\n","    # of digit classes in the latent space\n","    grid_x = np.linspace(a, b, n)\n","    grid_y = np.linspace(a, b, n)[::-1]\n","    \n","    for i, yi in enumerate(grid_y):\n","        for j, xi in enumerate(grid_x):\n","            z_sample = np.array([[xi, yi]])\n","            X_decoded = decoder_model.predict(z_sample)\n","            digit = X_decoded[0].reshape(digit_size, digit_size)\n","            figure[i * digit_size: (i + 1) * digit_size,\n","                   j * digit_size: (j + 1) * digit_size] = digit\n","\n","    start_range    = digit_size // 2\n","    end_range      = n * digit_size + start_range + 1\n","    pixel_range    = np.arange(start_range, end_range, digit_size)\n","    sample_range_x = np.round(grid_x, 1)\n","    sample_range_y = np.round(grid_y, 1)\n","\n","    fig = plt.figure(figsize=figsize)\n","    plt.xticks(pixel_range, sample_range_x)\n","    plt.yticks(pixel_range, sample_range_y)\n","    plt.xlabel(\"z[0]\")\n","    plt.ylabel(\"z[1]\")\n","    plt.imshow(figure, cmap='Greys_r')\n","\n","    if save:\n","        plt.savefig(filename)\n","        plt.close(fig)\n","    else:\n","        print('range:(',a,':',b,')')\n","        plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nF4-ZTuvVbnK"},"source":["---\n","### d. Save Image Callback\n","\n","This class defines a Keras callback to save interpolating image generated each training epoch"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_GcWbmKyVfO2","colab":{}},"source":["class SaveImage(tf.keras.callbacks.Callback):\n","    def __init__(self, decoder=None, base_dir=None):\n","        super(SaveImage, self).__init__()\n","        self.decoder = decoder\n","        self.base_dir = base_dir\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        filename = self.base_dir+'/image'+str(epoch)+'.png'\n","        plot_interpolating(self.decoder, z_range=(-1,1), n=5,\n","                           save=True, filename=filename, \n","                           figsize=(5,5))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkv_4ClxTvcX","colab_type":"text"},"source":["---\n","### e. Plot History"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jpiIWr5S_P0c","colab":{}},"source":["def plot_history(history):\n","    plt.rcParams['figure.figsize'] = [6, 4]\n","    plt.plot(ae_hist.history['loss'])\n","    plt.plot(ae_hist.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Val'])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6Uit1y7BUYzK"},"source":["---\n","### f. Generate GIF\n","\n","This function generates a GIF animation from the saved images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jOFuzDwAUxVL","colab":{}},"source":["def show_gif(base_dir, anim_file):    \n","    with imageio.get_writer(anim_file, mode='I') as writer:\n","        filenames = glob.glob(base_dir+'/image*.png')\n","        filenames = sorted(filenames)\n","        last = -1\n","        for i,filename in enumerate(filenames):\n","            frame = (i**0.5)\n","            if round(frame) > round(last):\n","                last = frame\n","            else:\n","                continue\n","            image = imageio.imread(filename)\n","            writer.append_data(image)\n","            writer.append_data(image)\n","        image = imageio.imread(filename)\n","        writer.append_data(image)\n","    print('GIF saved as', anim_file)\n","    \n","    with open(anim_file,'rb') as f:\n","      display(Image(data=f.read(), format='png'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6lfzFC4hco1n"},"source":["---\n","---\n","# [Part 1] Convolutional AutoEncoder\n","\n","Now let's build our Convolutional AutoEncoder\n","\n","If the input is denoted by $x$, the encoder $E$ and the decoder $D$, the reconstruction is $\\hat{x} = D(E(x))$. In order to encourage reconstruction, we will minimize the mean squared error\n","\n","<center>\n","<img src='https://i.ibb.co/1bfHYBS/ae2.png' width=50%>\n","</center>\n","\n","\n","\n","$$\n","loss = ||x-\\hat{x}||^2 = ||x-d(z)||^2 = ||x-d(e(x))||^2\n","$$\n","\n","<br />\n","\n","The space of representations is often called the latent space. We are interested in AEs as this latent space can potentially be a smaller dimensional and better representation of our data. We may also generate new data examples with an autoencoder, but let's return to this later.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gy7jHIYgco16"},"source":["---\n","## 1 - Conv-AE Encoder\n","\n","First, define the Encoder model\n","> <font color='red'>**EXERCISE:** </font> Complete the Encoder\n","\n","The architecture is as follow:\n","<pre>\n","    * the input shape is 3-dimensional <font color='blue'><b>(28,28,1)</b></font>  \n","    * <b>conv</b> layer with <font color='blue'><b>16</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * flatten layer\n","    * <b>dense</b> layer with <font color='blue'><b>16</b></font> neurons using relu activation\n","    * output <b>dense</b> layer with <font color='blue'><b>2</b></font> neuron without activation\n","</pre>"]},{"cell_type":"code","metadata":{"id":"G7Dv1Kw3uCmE","colab_type":"code","colab":{}},"source":["input_shape = (image_size, image_size, 1)\n","latent_dim  = 2\n","\n","# create Input() with shape of input_shape\n","encoder_input = Input(shape=input_shape, name='encoder_input')\n","\n","# add conv2d layer to encoder_input with 16 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??(encoder_input)\n","\n","# add conv2d layer to x with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??\n","\n","# add flatten layer to x \n","x = ??\n","\n","# add dense layer to x with 16 neurons and relu activation\n","x = ??\n","\n","# add output layer to x using dense layer with latent_dim neurons, without activation\n","encoder_output = Dense(latent_dim, name='encoder_output')(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9hYx4sLjVElL"},"source":["---\n","\n","Then to instantiate encoder model\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2vd2Jci1fiWC","colab":{}},"source":["ae_encoder = Model(encoder_input, encoder_output, name='ae_encoder')\n","\n","ae_encoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2TlF8PHkuXYO"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EwJSDv_SD4dH","colab":{}},"source":["plot_model(ae_encoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QBx8ND_Rco2F"},"source":["---\n","## 2 - Conv-AE Decoder\n","\n","Next, we define the Decoder model\n","\n","> <font color='red'>**EXERCISE:** </font> Complete the Decoder\n","\n","The architecture is as follow:\n","<pre>\n","    * the input shape is 1-dimensional <font color='blue'><b>(2,)</b></font>  \n","    * <b>dense</b> layer with <font color='blue'><b>7*7*32</b></font> neurons using relu activation\n","    * <b>reshape</b> layer to convert input into <font color='blue'><b>(7,7,32)</b></font> 3-dimensional matrix\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>16</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * output <b>conv2dtranspose</b> layer with <font color='blue'><b>1</b></font> filters of <font color='blue'><b>3x3</b></font>, and <b>sigmoid</b> activation\n","</pre>"]},{"cell_type":"code","metadata":{"id":"hyggZiaGuCma","colab_type":"code","colab":{}},"source":["# create Input() with shape of latent_dim\n","decoder_input = Input(shape=(latent_dim,), name='decoder_input')\n","\n","\n","# add dense layer to decoder_input with 7*7*32 neurons and relu activation\n","x = ??(decoder_input)\n","\n","# add reshape layer to x to reshape back into (7, 7, 32)\n","x = ??\n","\n","# add con2dtranspose to x with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??\n","\n","# add con2dtranspose to x with 16 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??\n","\n","\n","# add output layer to x using con2dtranspose layer with 1 filter, \n","# kernel size 3, sigmoid activation, and padding same\n","decoder_output = Conv2DTranspose(filters=1, kernel_size=3, activation='sigmoid', \n","                                 padding='same', name='decoder_output')(x)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lv2qwvsvXcBF"},"source":["---\n","\n","Then to instantiate decoder model\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BRAanLi7fjOw","colab":{}},"source":["ae_decoder = Model(decoder_input, decoder_output, name='ae_decoder')\n","\n","ae_decoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XARcYXAhubS6"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"91LRuKsYEEJt","colab":{}},"source":["plot_model(ae_decoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"VUe3F9PFco2K"},"source":["---\n","## 3 - Conv-AE Complete \n","\n","Lastly, we combine the Encoder and Decoder into single AutoEncoder model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Q-1fnu-1co2M","colab":{}},"source":["# call ae_encoder() function with input encoder_input\n","encoded        = ae_encoder(encoder_input)\n","\n","# call ae_decoder() function with input encoded\n","decoder_output = ae_decoder(encoded)\n","\n","# call Model() function with input encoder_input and decoder_output\n","autoenc = Model(encoder_input, decoder_output, name='conv_autoencoder')\n","\n","autoenc.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XZ5UnJBfucFx"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HwFOid-8EP4T","colab":{}},"source":["plot_model(autoenc, show_shapes=True, show_layer_names=False, dpi=55, rankdir='LR', expand_nested=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sjZBkUvQco2P"},"source":["---\n","## 4 - Train Convolutional AutoEncoder\n","\n","The next step is to train the model"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gAp5G0z_co2d"},"source":["---\n","First let's compile using the model using `rmsprop` optimizer and `binary_crossentropy` loss\n","\n","> <font color='red'>**EXERCISE:** </font> Try using other optimizer"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wXeYWg-4co2f","colab":{}},"source":["autoenc.compile(optimizer='rmsprop', loss='binary_crossentropy')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i_bkV0BWco2j"},"source":["---\n","\n","Then train the model for 30 epochs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_qyMiVleco2k","scrolled":true,"colab":{}},"source":["batch_size  = 256\n","epochs      = 30\n","\n","base_dir = 'conv_ae'\n","tf.io.gfile.mkdir(base_dir)\n","myCallback = SaveImage(ae_decoder, base_dir)\n","\n","logdir = os.path.join(base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","ae_hist = autoenc.fit(X_train, X_train,\n","                      epochs=epochs,\n","                      batch_size=batch_size,\n","                      validation_data=(X_test, X_test),\n","                      callbacks = [myCallback, tensorboard_callback],\n","                      verbose=2\n","                      )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5NlSaTzb-4_g"},"source":["---\n","Visualize Loss History"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6vc8XkSr-7wx","colab":{}},"source":["plot_history(ae_hist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eFOewUYA_dxb","colab":{}},"source":["%tensorboard --logdir conv_ae"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mhYqRboTco2o"},"source":["---\n","## 5 - Visualize Generated Images\n","\n","Now let's visualize the result"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nePogLatco2p"},"source":["---\n","### a. Latent Space Distribution\n","\n","Plots labels and MNIST digits as function of 2-dim latent vector\n","\n","You should see that the distribution of latent vector is clustered by class as AutoEncoder is intended to perform Dimensionality Reduction or Feature Extraction\n","\n","The clustered distribution is good for Classification purposes, but not for Image Generation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IxzT2Gdcco2u","colab":{}},"source":["batch_size=128\n","data_test = (X_test, y_test)\n","plot_latent_space(ae_encoder, data_test, batch_size=batch_size, vae=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FcmxJxT140z_"},"source":["---\n","### b. Reconstructed Images\n","\n","Next, let's visualize the Reconstructed Images from several data test\n","\n","You'll see that using shallow ConvNet and several epochs, we can already perform dimensionality reduction, and reconstruct the reduced data back into its original image"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ckivKIeUHtif","colab":{}},"source":["ori_img = X_test[:10]\n","rec_img = autoenc.predict(ori_img, verbose=0)\n","\n","fig, ax = plt.subplots(2,10,figsize=(15,4.5))\n","fig.subplots_adjust(hspace=0.1, wspace=0.1)\n","for i in range(10):\n","    ax[0,i].imshow(ori_img[i].reshape(28, 28), cmap='gray')\n","    ax[0,i].set_title(y_test[i])\n","    ax[0,i].axis('off')\n","    ax[1,i].imshow(rec_img[i].reshape(28, 28), cmap='gray')\n","    ax[1,i].axis('off')\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"I9e6186O46VY"},"source":["---\n","### c. Randomly Generated\n","\n","Now let's try to generate &nbsp;$5$&nbsp; new images from a random latent vector &nbsp;$z$&nbsp;\n","\n","You should see that the result is quite good\n","\n","The network is able to generate new image from random input"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6ITBoNbjeHUI","colab":{}},"source":["generate_image(ae_decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"En0SFKRM3KbA"},"source":["But what happened if we try to widen the vector random range to $(-5..5)$?\n","\n","You might see that some images get corrupted because the decoder model is not trained to generate images from that input area"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"izZWRt2E3I93","colab":{}},"source":["generate_image(ae_decoder, z_range=(-5,5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_Pi3rqMf4Lz3"},"source":["You can try to generate single image by running the cell below\n","\n","you can use a manually selected latent vector or generate a random vector"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2oIlnYH1ekva","colab":{}},"source":["z_sample = np.random.uniform(-10, 10, (1, 2))\n","# z_sample = np.array([[0,0]])\n","\n","X_decoded = ae_decoder.predict(z_sample)\n","digit = X_decoded.reshape(28, 28)\n","plt.imshow(digit, cmap='gray')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"e-jufV7zco2x"},"source":["---\n","### d. Plot Interpolation\n","\n","Now let's generate the image interpolation generated from a range of latent vectors\n","\n","First we generate $10\\times 10$ image ranged from $(-1..1)$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CbqFYuLRco22","colab":{}},"source":["plot_interpolating(ae_decoder, n=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JKRBy8oM4z8D"},"source":["Now let's try to widen the range. For that we deepen the interpolation to $20\\times 20$\n","\n","Again, you may notice that in some input ranges, the generated images begin to be unrecognizable"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QBj_cDV6hhUo","colab":{}},"source":["plot_interpolating( ae_decoder, n=20, z_range=(-10,10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ikYqrmMVUj16"},"source":["---\n","### e. Generate GIF\n","\n","Now this is just for fun, we combine the saved image generated each epoch while training into a GIF animation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MwLP7vdEXNqg","colab":{}},"source":["base_dir = 'conv_ae'\n","\n","show_gif(base_dir, 'conv_autoenc.gif')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"31qoS0UTgSzz"},"source":["---\n","---\n","# [Part 2] Variational AutoEncoder\n","\n","As already mentioned before, the basic idea behind a **Variational Autoencoder** is that instead of mapping an input to fixed vector, input is mapped to a distribution. \n","\n","A variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process.\n","\n","<p align='center'>\n","<img src=\"https://i.ibb.co/k1hNtgs/vae3.png\" width=65% />\n","\n","\n","The only difference between the autoencoder and variational autoencoder is that bottleneck vector is replaced with two different vectors one representing the mean of the distribution and the other representing the standard deviation of the distribution.\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_AfoEeoF1ta7"},"source":["---\n","## 0 - Random Sampling Function\n","\n","Function below is a helper function to generate a random latent vector &nbsp;$z$&nbsp; from input &nbsp;$mean$&nbsp; and &nbsp;$variance$"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y0oNxGlGfvff","colab":{}},"source":["def sampling(args):\n","    \n","    z_mean, z_log_var = args\n","\n","    batch = K.shape(z_mean)[0]\n","    dim = K.int_shape(z_mean)[1]\n","\n","    # by default, random_normal has mean=0 and std=1.0\n","    epsilon = K.random_normal(shape=(batch, dim))\n","\n","    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FX5dojpagd5K"},"source":["---\n","## 1 - VAE Encoder\n","\n","This defines the generative model which takes a latent encoding as input, and outputs the parameters for a conditional distribution of the observation, i.e. $p(x|z)$. Additionally, we use a unit Gaussian prior $p(z)$ for the latent variable.\n","\n","<center>\n","<img src='https://i.ibb.co/4pbr5rL/vae4.png' width=40%>\n","</center>\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iiCNoyWmDuvW"},"source":["\n","Now to define the Encoder model\n","\n","> <font color='red'>**EXERCISE:** </font> Complete the Encoder\n","\n","The architecture is as follow:\n","<pre>\n","    * the input shape is 3-dimensional <font color='blue'><b>(28,28,1)</b></font>  \n","    * <b>conv</b> layer with <font color='blue'><b>16</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * flatten layer\n","    * <b>dense</b> layer with <font color='blue'><b>16</b></font> neurons using relu activation\n","\n","    * output 1: <b>dense</b> layer with <font color='blue'><b>2</b></font> neuron without activation\n","    * output 2: <b>dense</b> layer with <font color='blue'><b>2</b></font> neuron without activation\n","</pre>"]},{"cell_type":"code","metadata":{"id":"Bk7GQ7wGuCnd","colab_type":"code","colab":{}},"source":["input_shape = (image_size, image_size, 1)\n","latent_dim  = 2\n","\n","# create Input() with shape of input_shape\n","v_encoder_input = Input(shape=input_shape, name='v_encoder_input')\n","\n","# add conv2d layer to v_encoder_input with 16 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??(v_encoder_input)\n","\n","# add conv2d layer to x with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??\n","\n","# add flatten layer to x \n","x = ??\n","\n","# add dense layer to x with 16 neurons and relu activation\n","x = ??\n","\n","# ---\n","\n","# generate latent vector Q(z|X)\n","z_mean    = Dense(latent_dim, name='z_mean')(x)\n","\n","# add first output layer to x \n","z_log_var = Dense(latent_dim, name='z_log_var')(x)\n","\n","# generate random sampling from z_mean and z_log_var \n","z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n","\n","# combine all outputs into a single list\n","v_encoder_output = [z_mean, z_log_var, z]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5ngooQzEbVgm"},"source":["---\n","\n","Then to instantiate encoder model\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tYZuDR-zi9VJ","colab":{}},"source":["vae_encoder = Model(v_encoder_input, v_encoder_output, name='vae_encoder')\n","\n","vae_encoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5Lp3MxS9FXFB"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n8XGLD7Xi9VT","colab":{}},"source":["plot_model(vae_encoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sfxCBy1UggRD"},"source":["---\n","## 2 - VAE Decoder\n","\n","Then we define the Decoder model exactly the same as the previous Vanilla Decoder\n","\n","> <font color='red'>**EXERCISE:** </font> Complete the Decoder\n","\n","The architechture is as follow:\n","<pre>\n","    * the input shape is 1-dimensional <font color='blue'><b>(2,)</b></font>  \n","    * <b>dense</b> layer with <font color='blue'><b>7*7*32</b></font> neurons using relu activation\n","    * <b>reshape</b> layer to convert input into <font color='blue'><b>(7,7,32)</b></font> 3-dimensional matrix\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>32</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * <b>conv2dtranspose</b> layer with <font color='blue'><b>16</b></font> filters of <font color='blue'><b>3x3</b></font>, using stride <font color='blue'><b>2</b></font>, and relu activation\n","    * output <b>conv2dtranspose</b> layer with <font color='blue'><b>1</b></font> filters of <font color='blue'><b>3x3</b></font>, and <b>sigmoid</b> activation\n","</pre>"]},{"cell_type":"code","metadata":{"id":"FJWcqtGuuCns","colab_type":"code","colab":{}},"source":["# create Input() with shape of latent_dim\n","# set the name as 'v_decoder_input'\n","v_decoder_input = Input(shape=(latent_dim,), name='v_decoder_input')\n","\n","\n","# add dense layer to v_decoder_input with 7*7*32 neurons and relu activation\n","x = ?? (v_decoder_input)\n","\n","# add reshape layer to x to reshape back into (7, 7, 32)\n","x = ??\n","\n","# add con2dtranspose to x with 32 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??\n","\n","# add con2dtranspose to x with 16 filters, kernel size 3, and strides 2\n","# use relu activation and padding same\n","x = ??\n","\n","\n","# add output layer to x using con2dtranspose layer with 1 filter, \n","# kernel size 3, sigmoid activation, and padding same\n","v_decoder_output = Conv2DTranspose(filters=1, kernel_size=3, activation='sigmoid', \n","                                 padding='same', name='v_decoder_output')(x)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QC2bJNnwbmDf"},"source":["---\n","\n","Then to instantiate decoder model\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dNRCC2SkjLzI","colab":{}},"source":["vae_decoder = Model(v_decoder_input, v_decoder_output, name='vae_decoder')\n","\n","vae_decoder.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Y_Sxl1iVGUyR"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Z07BVJtzj0l4","colab":{}},"source":["plot_model(vae_decoder, show_shapes=True, show_layer_names=False, dpi=65, rankdir='LR')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yzs6R-Emj8wx"},"source":["---\n","## 3 - VAE Complete\n","\n","Now instantiate VAE model = VEncoder + VDecoder"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LhuNwFTFj8w8","colab":{}},"source":["# call vae_encoder() function with input v_encoder_input\n","v_encoded        = vae_encoder(v_encoder_input)[2]\n","\n","# call vae_decoder() function with input v_encoded\n","v_decoder_output = vae_decoder(v_encoded)\n","\n","# call Model() function with input v_encoder_input and v_decoder_output\n","vae = Model(v_encoder_input, v_decoder_output, name='variational_autoencoder')\n","\n","vae.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"69RLp3cBLYXZ"},"source":["Visualize the network architecture"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ta0pIQacj8xD","colab":{}},"source":["plot_model(vae, show_shapes=True, show_layer_names=False, dpi=55, rankdir='LR', expand_nested=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6o0NpsCLil7Y"},"source":["---\n","## 4 - Train Variational AutoEncoder\n","\n","Next to train the Variational AutoEncoder"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X3Phn362isEL"},"source":["---\n","### a. Define Loss\n","\n","<center>\n","<img src='https://i.ibb.co/bLLWkcy/vae5.png' width=50%>\n","</center>\n","\n","\n","\\begin{align}\n","loss & = C||x-\\hat{x}||^2 + KL[\\mathcal{N}(\\mu_x, \\sigma_x), \\mathcal{N}(0, I) ]\\\\\\\\\n","& = C||x-f(x)||^2 + KL[\\mathcal{N}(g(x), h(x)), \\mathcal{N}(0, I) ]\\\\\\\\\n","\\end{align}\n","\n","---\n","\n","VAEs train by maximizing the evidence lower bound (ELBO) on the marginal log-likelihood:\n","\n","$$\\log p(x) \\ge \\text{ELBO} = \\mathbb{E}_{q(z|x)}\\left[\\log \\frac{p(x, z)}{q(z|x)}\\right].$$\n","\n","then we calculate the KL term to optimize it, thus we have\n","$$\n","D_{KL} = \\frac{1}{2}\\sum_k\\Big(\\exp(\\Sigma(X))+\\mu^2(X)-1-\\Sigma(X)\\Big)\n","$$\n","\n","Now to calculate the total loss is defined by\n","\n","    VAE loss = mse_loss + kl_loss\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KDZoXcRuit0s","colab":{}},"source":["from tensorflow.keras.losses import mse\n","\n","# calculate construction loss\n","reconstruction_loss = mse(K.flatten(v_encoder_input), K.flatten(v_decoder_output))\n","reconstruction_loss *= 28 * 28\n","\n","# calculate KL Loss\n","kl_loss = K.exp(z_log_var) + K.square(z_mean) - 1 - z_log_var\n","kl_loss = K.sum(kl_loss, axis=-1)\n","kl_loss *= 0.5\n","\n","# calculate VAE Loss\n","vae_loss = K.mean(reconstruction_loss + kl_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wsuuu0LRiyDM"},"source":["---\n","### b. Compile Model\n","Add loss to model and compile it"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aZrOdUFncQB3","colab":{}},"source":["vae.add_loss(vae_loss)\n","\n","vae.compile(optimizer='rmsprop')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"tLRDRYxPi4RO"},"source":["---\n","### c. Train DC-VAE"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LT450MTpi63X","scrolled":true,"colab":{}},"source":["batch_size  = 256\n","epochs      = 30\n","\n","base_dir = 'var_ae'\n","tf.io.gfile.mkdir(base_dir)\n","myCallback = SaveImage(vae_decoder, base_dir)\n","\n","logdir = os.path.join(base_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","vae_hist = vae.fit(X_train, \n","                   epochs=epochs,\n","                   batch_size=batch_size,\n","                   validation_data=(X_test, None),\n","                   callbacks = [myCallback, tensorboard_callback],\n","                   verbose=2\n","                   )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y2wghfCCA8Z9"},"source":["---\n","### d. Visualize Loss History"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dlNd9WcHA8aE","colab":{}},"source":["plot_history(vae_hist)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ie4bpjVg9LJx","colab":{}},"source":["%tensorboard --logdir var_ae"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jKoQgW1-NvqA"},"source":["---\n","## 5 - Visualize Generated Images\n","\n","Now let's visualize the result"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"i0rE6yjRNvqJ"},"source":["---\n","### a. Latent Space Distribution\n","\n","Plots labels and MNIST digits as function of 2-dim latent vector\n","\n","You should see that now the latent vector is more smoothly distributed across the space"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YZXBcOt2vfza","colab":{}},"source":["batch_size=128\n","data_test = (X_test, y_test)\n","\n","plot_latent_space(vae_encoder, data_test, batch_size=batch_size, vae=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QeQQV23YvoUR"},"source":["---\n","### b. Randomly Generated\n","\n","Now we skip directly to generate &nbsp;$5$&nbsp; new images from a random latent vector &nbsp;$z$&nbsp;\n","\n","You should see that the result is better\n","\n","The network is able to generate new image from random input"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y4BvOfShvoUT","colab":{}},"source":["generate_image(vae_decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rhqp1oMqOJeB"},"source":["But what happened if we try to widen the vector random range to $(-5..5)$?\n","\n","You might see that the image is much better that using Vanilla AutoEncoder, even though the network is not trained to generate image from that range"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FRME7oCVOPeL","colab":{}},"source":["generate_image(vae_decoder, z_range=(-5,5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lQSmciuVONJN"},"source":["You can try to generate single image by running the cell below\n","\n","you can use predefined latent vector or generate a random vector"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"462RacFDvoUX","colab":{}},"source":["z_sample = np.array([[0,0]])\n","X_decoded = vae_decoder.predict(z_sample)\n","digit = X_decoded.reshape(28, 28)\n","plt.imshow(digit, cmap='gray')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9PnCFSNAOkGg"},"source":["---\n","### c. Plot Interpolation\n","\n","Next let's generate the image interpolation generated from a range of latent vectors\n","\n","First we generate $10\\times 10$ image ranged from $(-1..1)$\n","\n","You may see that the interpolation is done more smoothly"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MsIx4Zh4voUc","colab":{}},"source":["plot_interpolating(vae_decoder, n=10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h32dOssyOofg"},"source":["Now let's try to widen the range. For that we deepen the interpolation to $20\\times 20$\n","\n","You might see that the interpolation still results smooth images across wide range"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BfoD4trSvoUg","colab":{}},"source":["plot_interpolating( vae_decoder, n=20, z_range=(-5,5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hB-U6_GYO0qR"},"source":["---\n","### d. Generate GIF\n","\n","Now this is just for fun, we combine the saved image generated each epoch while training into a GIF animation\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"riB6_ZtvvoUm","colab":{}},"source":["base_dir = 'var_ae'\n","\n","show_gif(base_dir, 'var_autoenc.gif')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"quMA17JKDnpI"},"source":["---\n","\n","# Congratulation\n","\n","<font size=5> You've Completed Practical 5</font>\n","\n","<p>Copyright &copy;  <a href=https://www.linkedin.com/in/andityaarifianto/>2020 - ADF</a> </p>"]}]}